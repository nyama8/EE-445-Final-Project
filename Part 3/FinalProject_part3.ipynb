{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 445 Final Project (Part 3): Modeling effectiveness of government response\n",
    "Nicholas Yama, Spring 2020\n",
    "\n",
    "## Project goal\n",
    "\n",
    "At the time of writing (07 May 2020) the United States is beginning to see the number of cases slowing, resulting in increase in desire by a vocal minority to return to normal operations. This would involve the uplifing of wide-scale quarantine and stay-at-home orders, of which the evidence indicates has done significant good in slowing the spread of infections. Many arguments are being levied against the effectiveness of these restrictions which are often of dubious scientific merit.\n",
    "\n",
    "The effectiveness of the government response has varied between states in curbing the number of cases and, due to the lack of national policy from the federal government, will likey be exacerbated as individual states begin to relax these measures. The goal of this project is to model the effectiveness of the government's response at a state level in terms of initial growth rates of the infection.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "The number of infected individuals $N$ in its early stages generally can be modeled by an exponential curve\n",
    "\n",
    "$$N=\\exp(r(t-t_0))$$\n",
    "\n",
    "where $r$ is the growth rate, $t$ is the time, and $t_0$ is the time until the first infection. The growth rate $r$ can then be determined by a linear regression on $\\ln(N) = r(t - t_0)$.\n",
    "\n",
    "This growth rate $r$ will be predicted using a linear regression-type network for the first 6 weeks starting March 1st (a day later the US would reach the first confirmed 100 cases).\n",
    "\n",
    "The parameters of interest include:\n",
    "1. The testing rate integrated weekly (so the number of tests administered from March 1 to a given week) normalized by the population (# tests adminstered/population, for 6 weeks).\n",
    "2. Time from March 1 to declare a \"state of emergency\"\n",
    "3. Time from March 1 to issue a stay-at-home order (state level)\n",
    "4. Maximum number of persons at a gathering (in cases where a distinction is made between private and public gatherings, the maximum is taken)\n",
    "5. Travel restrictions (1 = none, 2 = partial, 3 = mandatory).\n",
    "6. School closures (1 = none, 2 = partial, 3 = all).\n",
    "7. Daycare closures (1 = none, 2 = partial, 3 = all).\n",
    "8. Restauraunts closures (1 = none, 2 = partial, 3 = all).\n",
    "8. Non-essential retail closures (1 = none, 2 = partial, 3 = all).\n",
    "9. Governor political affiliation (-1 = republican, 1 = democrat).\n",
    "10. Population density (per square mile, control parameter).\n",
    "\n",
    "When parameters associated with time of response were not achieved within the time frame provided (eg. no stay at home order issued within the 6 weeks) a value of 50 is assigned (greater than maximal value of 42).\n",
    "\n",
    "Each data point will represent an individual state. Only the 50 states and the District of Columbia will be considered, territories of the US such as American Samoa and Puerto Rico will not be used.\n",
    "\n",
    "### Data sources\n",
    "[1] Target data to calculate the growth rate of cases, and the testing data was obtained via the COVID Tracking project: https://covidtracking.com/. The data was collected on May 06 (but data was only used up to April 12 so the collection date does not matter).\n",
    "\n",
    "[2] Parameters 2-9 were obtained from the Wikipedia page: https://en.wikipedia.org/wiki/U.S._state_and_local_government_response_to_the_COVID-19_pandemic, I processed the data in Microsoft Excel, using a search and replace on terms corresponding to the values above.\n",
    "\n",
    "[3] Governor partisanship (parameter 9) was found: https://en.wikipedia.org/wiki/List_of_United_States_governors\n",
    "\n",
    "[4] State population statistics was found from the US 2010 Census projections for 2019. The full dataset can be found: https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-total.html\n",
    "\n",
    "# Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tests 1</th>\n",
       "      <th>Tests 2</th>\n",
       "      <th>Tests 3</th>\n",
       "      <th>Tests 4</th>\n",
       "      <th>Tests 5</th>\n",
       "      <th>Tests 6</th>\n",
       "      <th>Tests 7</th>\n",
       "      <th>density</th>\n",
       "      <th>emergency</th>\n",
       "      <th>quarantine</th>\n",
       "      <th>gatherings</th>\n",
       "      <th>travel</th>\n",
       "      <th>school</th>\n",
       "      <th>daycares</th>\n",
       "      <th>restauraunts</th>\n",
       "      <th>retail</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>0.145489</td>\n",
       "      <td>1.380985</td>\n",
       "      <td>8.824113</td>\n",
       "      <td>30.812967</td>\n",
       "      <td>63.724578</td>\n",
       "      <td>102.464746</td>\n",
       "      <td>412.058824</td>\n",
       "      <td>8.921569</td>\n",
       "      <td>29.941176</td>\n",
       "      <td>11.588235</td>\n",
       "      <td>1.862745</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.764706</td>\n",
       "      <td>2.941176</td>\n",
       "      <td>2.725490</td>\n",
       "      <td>1.490196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.020440</td>\n",
       "      <td>0.454149</td>\n",
       "      <td>3.618019</td>\n",
       "      <td>11.500424</td>\n",
       "      <td>27.779183</td>\n",
       "      <td>54.629108</td>\n",
       "      <td>90.068199</td>\n",
       "      <td>1536.591050</td>\n",
       "      <td>3.632317</td>\n",
       "      <td>9.213928</td>\n",
       "      <td>13.661883</td>\n",
       "      <td>0.916943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.586114</td>\n",
       "      <td>0.310597</td>\n",
       "      <td>0.634931</td>\n",
       "      <td>0.543049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014126</td>\n",
       "      <td>0.420049</td>\n",
       "      <td>2.752207</td>\n",
       "      <td>6.704623</td>\n",
       "      <td>11.529469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006828</td>\n",
       "      <td>0.246191</td>\n",
       "      <td>2.712900</td>\n",
       "      <td>15.046188</td>\n",
       "      <td>37.274537</td>\n",
       "      <td>58.549733</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047543</td>\n",
       "      <td>0.412394</td>\n",
       "      <td>6.138395</td>\n",
       "      <td>25.719804</td>\n",
       "      <td>49.315151</td>\n",
       "      <td>79.901797</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121059</td>\n",
       "      <td>1.170444</td>\n",
       "      <td>9.018394</td>\n",
       "      <td>38.097159</td>\n",
       "      <td>69.805266</td>\n",
       "      <td>114.818142</td>\n",
       "      <td>226.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.145767</td>\n",
       "      <td>3.226572</td>\n",
       "      <td>25.359516</td>\n",
       "      <td>60.308136</td>\n",
       "      <td>170.019023</td>\n",
       "      <td>379.349636</td>\n",
       "      <td>640.796553</td>\n",
       "      <td>11011.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tests 1    Tests 2    Tests 3    Tests 4     Tests 5     Tests 6  \\\n",
       "count  51.000000  51.000000  51.000000  51.000000   51.000000   51.000000   \n",
       "mean    0.003220   0.145489   1.380985   8.824113   30.812967   63.724578   \n",
       "std     0.020440   0.454149   3.618019  11.500424   27.779183   54.629108   \n",
       "min     0.000000   0.000000   0.014126   0.420049    2.752207    6.704623   \n",
       "25%     0.000000   0.006828   0.246191   2.712900   15.046188   37.274537   \n",
       "50%     0.000000   0.047543   0.412394   6.138395   25.719804   49.315151   \n",
       "75%     0.000000   0.121059   1.170444   9.018394   38.097159   69.805266   \n",
       "max     0.145767   3.226572  25.359516  60.308136  170.019023  379.349636   \n",
       "\n",
       "          Tests 7       density  emergency  quarantine  gatherings     travel  \\\n",
       "count   51.000000     51.000000  51.000000   51.000000   51.000000  51.000000   \n",
       "mean   102.464746    412.058824   8.921569   29.941176   11.588235   1.862745   \n",
       "std     90.068199   1536.591050   3.632317    9.213928   13.661883   0.916943   \n",
       "min     11.529469      1.000000  -1.000000   18.000000    1.000000   1.000000   \n",
       "25%     58.549733     47.500000   7.500000   23.000000    1.000000   1.000000   \n",
       "50%     79.901797    106.000000  10.000000   27.000000   10.000000   2.000000   \n",
       "75%    114.818142    226.500000  12.000000   32.500000   10.000000   3.000000   \n",
       "max    640.796553  11011.000000  15.000000   50.000000   50.000000   3.000000   \n",
       "\n",
       "       school   daycares  restauraunts     retail      party  \n",
       "count    51.0  51.000000     51.000000  51.000000  51.000000  \n",
       "mean      3.0   2.764706      2.941176   2.725490   1.490196  \n",
       "std       0.0   0.586114      0.310597   0.634931   0.543049  \n",
       "min       3.0   1.000000      1.000000   1.000000   0.000000  \n",
       "25%       3.0   3.000000      3.000000   3.000000   1.000000  \n",
       "50%       3.0   3.000000      3.000000   3.000000   2.000000  \n",
       "75%       3.0   3.000000      3.000000   3.000000   2.000000  \n",
       "max       3.0   3.000000      3.000000   3.000000   2.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load parameters dataset\n",
    "url = 'https://raw.githubusercontent.com/nyama8/EE-445-Final-Project/master/Processed%20data/sampledata.csv'\n",
    "datadf = pd.read_csv(url, error_bad_lines=False)\n",
    "datadf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.216806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.035466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.134223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.190891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.219613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.239307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.299372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rate\n",
       "count  51.000000\n",
       "mean    0.216806\n",
       "std     0.035466\n",
       "min     0.134223\n",
       "25%     0.190891\n",
       "50%     0.219613\n",
       "75%     0.239307\n",
       "max     0.299372"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load target dataset\n",
    "url = 'https://raw.githubusercontent.com/nyama8/EE-445-Final-Project/master/Processed%20data/sampletarget.csv'\n",
    "targetdf = pd.read_csv(url, error_bad_lines=False)\n",
    "targetdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datadf.to_numpy()\n",
    "target = targetdf.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with a feed-forward network\n",
    "\n",
    "As an intial pass at this problem, I used linear regression to model the relation between the parameters and infection rate $r$. This would represent a network with 13 input nodes and a single output node with a mean squared error loss function. Here I use a more advanced feed-forward network approach to address the problem.\n",
    "\n",
    "In the following approaches, I used the tutorial available here: https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/, to help with syntax and how to actually implement the network.\n",
    "\n",
    "\n",
    "## Recreating explicit linear regression with a network\n",
    "Previously I used the LinearRegression() function which determines the optimal estimator directly. Here I implement a network with the same topology that approximates the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network model\n",
    "def base_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=17, kernel_initializer='normal'))\n",
    "    \n",
    "    model.layers[0].set_weights([np.zeros([17,1])+0.01, np.array([0.00])])\n",
    "\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/150\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 402.4555\n",
      "Epoch 2/150\n",
      "34/34 [==============================] - 0s 260us/step - loss: 158.0172\n",
      "Epoch 3/150\n",
      "34/34 [==============================] - 0s 361us/step - loss: 55.3846\n",
      "Epoch 4/150\n",
      "34/34 [==============================] - 0s 336us/step - loss: 12.8704\n",
      "Epoch 5/150\n",
      "34/34 [==============================] - 0s 298us/step - loss: 2.4755\n",
      "Epoch 6/150\n",
      "34/34 [==============================] - 0s 274us/step - loss: 2.6757\n",
      "Epoch 7/150\n",
      "34/34 [==============================] - 0s 283us/step - loss: 2.8060\n",
      "Epoch 8/150\n",
      "34/34 [==============================] - 0s 273us/step - loss: 1.4979\n",
      "Epoch 9/150\n",
      "34/34 [==============================] - 0s 348us/step - loss: 0.3848\n",
      "Epoch 10/150\n",
      "34/34 [==============================] - 0s 341us/step - loss: 0.0813\n",
      "Epoch 11/150\n",
      "34/34 [==============================] - 0s 294us/step - loss: 0.1203\n",
      "Epoch 12/150\n",
      "34/34 [==============================] - 0s 348us/step - loss: 0.1076\n",
      "Epoch 13/150\n",
      "34/34 [==============================] - 0s 391us/step - loss: 0.0431\n",
      "Epoch 14/150\n",
      "34/34 [==============================] - 0s 309us/step - loss: 0.0129\n",
      "Epoch 15/150\n",
      "34/34 [==============================] - 0s 308us/step - loss: 0.0142\n",
      "Epoch 16/150\n",
      "34/34 [==============================] - 0s 293us/step - loss: 0.0155\n",
      "Epoch 17/150\n",
      "34/34 [==============================] - 0s 342us/step - loss: 0.0116\n",
      "Epoch 18/150\n",
      "34/34 [==============================] - 0s 336us/step - loss: 0.0093\n",
      "Epoch 19/150\n",
      "34/34 [==============================] - 0s 492us/step - loss: 0.0090\n",
      "Epoch 20/150\n",
      "34/34 [==============================] - 0s 311us/step - loss: 0.0086\n",
      "Epoch 21/150\n",
      "34/34 [==============================] - 0s 325us/step - loss: 0.0078\n",
      "Epoch 22/150\n",
      "34/34 [==============================] - 0s 356us/step - loss: 0.0073\n",
      "Epoch 23/150\n",
      "34/34 [==============================] - 0s 372us/step - loss: 0.0069\n",
      "Epoch 24/150\n",
      "34/34 [==============================] - 0s 309us/step - loss: 0.0066\n",
      "Epoch 25/150\n",
      "34/34 [==============================] - 0s 405us/step - loss: 0.0063\n",
      "Epoch 26/150\n",
      "34/34 [==============================] - 0s 327us/step - loss: 0.0060\n",
      "Epoch 27/150\n",
      "34/34 [==============================] - 0s 343us/step - loss: 0.0058\n",
      "Epoch 28/150\n",
      "34/34 [==============================] - 0s 382us/step - loss: 0.0055\n",
      "Epoch 29/150\n",
      "34/34 [==============================] - 0s 314us/step - loss: 0.0053\n",
      "Epoch 30/150\n",
      "34/34 [==============================] - 0s 373us/step - loss: 0.0051\n",
      "Epoch 31/150\n",
      "34/34 [==============================] - 0s 316us/step - loss: 0.0049\n",
      "Epoch 32/150\n",
      "34/34 [==============================] - 0s 342us/step - loss: 0.0048\n",
      "Epoch 33/150\n",
      "34/34 [==============================] - 0s 279us/step - loss: 0.0046\n",
      "Epoch 34/150\n",
      "34/34 [==============================] - 0s 346us/step - loss: 0.0045\n",
      "Epoch 35/150\n",
      "34/34 [==============================] - 0s 335us/step - loss: 0.0043\n",
      "Epoch 36/150\n",
      "34/34 [==============================] - 0s 306us/step - loss: 0.0042\n",
      "Epoch 37/150\n",
      "34/34 [==============================] - 0s 331us/step - loss: 0.0041\n",
      "Epoch 38/150\n",
      "34/34 [==============================] - 0s 347us/step - loss: 0.0039\n",
      "Epoch 39/150\n",
      "34/34 [==============================] - 0s 397us/step - loss: 0.0038\n",
      "Epoch 40/150\n",
      "34/34 [==============================] - 0s 353us/step - loss: 0.0037\n",
      "Epoch 41/150\n",
      "34/34 [==============================] - 0s 300us/step - loss: 0.0036\n",
      "Epoch 42/150\n",
      "34/34 [==============================] - 0s 319us/step - loss: 0.0035\n",
      "Epoch 43/150\n",
      "34/34 [==============================] - 0s 283us/step - loss: 0.0034\n",
      "Epoch 44/150\n",
      "34/34 [==============================] - 0s 271us/step - loss: 0.0034\n",
      "Epoch 45/150\n",
      "34/34 [==============================] - 0s 298us/step - loss: 0.0033\n",
      "Epoch 46/150\n",
      "34/34 [==============================] - 0s 255us/step - loss: 0.0032\n",
      "Epoch 47/150\n",
      "34/34 [==============================] - 0s 312us/step - loss: 0.0031\n",
      "Epoch 48/150\n",
      "34/34 [==============================] - 0s 274us/step - loss: 0.0031\n",
      "Epoch 49/150\n",
      "34/34 [==============================] - 0s 242us/step - loss: 0.0030\n",
      "Epoch 50/150\n",
      "34/34 [==============================] - 0s 309us/step - loss: 0.0029\n",
      "Epoch 51/150\n",
      "34/34 [==============================] - 0s 298us/step - loss: 0.0029\n",
      "Epoch 52/150\n",
      "34/34 [==============================] - 0s 277us/step - loss: 0.0028\n",
      "Epoch 53/150\n",
      "34/34 [==============================] - 0s 287us/step - loss: 0.0028\n",
      "Epoch 54/150\n",
      "34/34 [==============================] - 0s 262us/step - loss: 0.0027\n",
      "Epoch 55/150\n",
      "34/34 [==============================] - 0s 266us/step - loss: 0.0027\n",
      "Epoch 56/150\n",
      "34/34 [==============================] - 0s 258us/step - loss: 0.0027\n",
      "Epoch 57/150\n",
      "34/34 [==============================] - 0s 263us/step - loss: 0.0026\n",
      "Epoch 58/150\n",
      "34/34 [==============================] - 0s 286us/step - loss: 0.0026\n",
      "Epoch 59/150\n",
      "34/34 [==============================] - 0s 264us/step - loss: 0.0025\n",
      "Epoch 60/150\n",
      "34/34 [==============================] - 0s 318us/step - loss: 0.0025\n",
      "Epoch 61/150\n",
      "34/34 [==============================] - 0s 282us/step - loss: 0.0025\n",
      "Epoch 62/150\n",
      "34/34 [==============================] - 0s 254us/step - loss: 0.0025\n",
      "Epoch 63/150\n",
      "34/34 [==============================] - 0s 314us/step - loss: 0.0024\n",
      "Epoch 64/150\n",
      "34/34 [==============================] - 0s 309us/step - loss: 0.0024\n",
      "Epoch 65/150\n",
      "34/34 [==============================] - 0s 287us/step - loss: 0.0024\n",
      "Epoch 66/150\n",
      "34/34 [==============================] - 0s 274us/step - loss: 0.0023\n",
      "Epoch 67/150\n",
      "34/34 [==============================] - 0s 259us/step - loss: 0.0023\n",
      "Epoch 68/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 0.0023\n",
      "Epoch 69/150\n",
      "34/34 [==============================] - 0s 301us/step - loss: 0.0023\n",
      "Epoch 70/150\n",
      "34/34 [==============================] - 0s 254us/step - loss: 0.0023\n",
      "Epoch 71/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 295us/step - loss: 0.0022\n",
      "Epoch 72/150\n",
      "34/34 [==============================] - 0s 279us/step - loss: 0.0022\n",
      "Epoch 73/150\n",
      "34/34 [==============================] - 0s 282us/step - loss: 0.0022\n",
      "Epoch 74/150\n",
      "34/34 [==============================] - 0s 383us/step - loss: 0.0022\n",
      "Epoch 75/150\n",
      "34/34 [==============================] - 0s 331us/step - loss: 0.0022\n",
      "Epoch 76/150\n",
      "34/34 [==============================] - 0s 241us/step - loss: 0.0021\n",
      "Epoch 77/150\n",
      "34/34 [==============================] - 0s 293us/step - loss: 0.0021\n",
      "Epoch 78/150\n",
      "34/34 [==============================] - 0s 291us/step - loss: 0.0021\n",
      "Epoch 79/150\n",
      "34/34 [==============================] - 0s 254us/step - loss: 0.0021\n",
      "Epoch 80/150\n",
      "34/34 [==============================] - 0s 311us/step - loss: 0.0021\n",
      "Epoch 81/150\n",
      "34/34 [==============================] - 0s 241us/step - loss: 0.0021\n",
      "Epoch 82/150\n",
      "34/34 [==============================] - 0s 320us/step - loss: 0.0020\n",
      "Epoch 83/150\n",
      "34/34 [==============================] - 0s 283us/step - loss: 0.0020\n",
      "Epoch 84/150\n",
      "34/34 [==============================] - 0s 237us/step - loss: 0.0020\n",
      "Epoch 85/150\n",
      "34/34 [==============================] - 0s 341us/step - loss: 0.0020\n",
      "Epoch 86/150\n",
      "34/34 [==============================] - 0s 322us/step - loss: 0.0020\n",
      "Epoch 87/150\n",
      "34/34 [==============================] - 0s 280us/step - loss: 0.0020\n",
      "Epoch 88/150\n",
      "34/34 [==============================] - 0s 334us/step - loss: 0.0020\n",
      "Epoch 89/150\n",
      "34/34 [==============================] - 0s 256us/step - loss: 0.0019\n",
      "Epoch 90/150\n",
      "34/34 [==============================] - 0s 240us/step - loss: 0.0019\n",
      "Epoch 91/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 0.0019\n",
      "Epoch 92/150\n",
      "34/34 [==============================] - 0s 262us/step - loss: 0.0019\n",
      "Epoch 93/150\n",
      "34/34 [==============================] - 0s 305us/step - loss: 0.0019\n",
      "Epoch 94/150\n",
      "34/34 [==============================] - 0s 310us/step - loss: 0.0019\n",
      "Epoch 95/150\n",
      "34/34 [==============================] - 0s 256us/step - loss: 0.0019\n",
      "Epoch 96/150\n",
      "34/34 [==============================] - 0s 283us/step - loss: 0.0019\n",
      "Epoch 97/150\n",
      "34/34 [==============================] - 0s 298us/step - loss: 0.0018\n",
      "Epoch 98/150\n",
      "34/34 [==============================] - 0s 285us/step - loss: 0.0018\n",
      "Epoch 99/150\n",
      "34/34 [==============================] - 0s 380us/step - loss: 0.0018\n",
      "Epoch 100/150\n",
      "34/34 [==============================] - 0s 287us/step - loss: 0.0018\n",
      "Epoch 101/150\n",
      "34/34 [==============================] - 0s 261us/step - loss: 0.0018\n",
      "Epoch 102/150\n",
      "34/34 [==============================] - 0s 321us/step - loss: 0.0018\n",
      "Epoch 103/150\n",
      "34/34 [==============================] - 0s 295us/step - loss: 0.0018\n",
      "Epoch 104/150\n",
      "34/34 [==============================] - 0s 255us/step - loss: 0.0018\n",
      "Epoch 105/150\n",
      "34/34 [==============================] - 0s 302us/step - loss: 0.0017\n",
      "Epoch 106/150\n",
      "34/34 [==============================] - 0s 249us/step - loss: 0.0017\n",
      "Epoch 107/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 0.0017\n",
      "Epoch 108/150\n",
      "34/34 [==============================] - 0s 290us/step - loss: 0.0017\n",
      "Epoch 109/150\n",
      "34/34 [==============================] - 0s 280us/step - loss: 0.0017\n",
      "Epoch 110/150\n",
      "34/34 [==============================] - 0s 320us/step - loss: 0.0017\n",
      "Epoch 111/150\n",
      "34/34 [==============================] - 0s 283us/step - loss: 0.0017\n",
      "Epoch 112/150\n",
      "34/34 [==============================] - 0s 250us/step - loss: 0.0017\n",
      "Epoch 113/150\n",
      "34/34 [==============================] - 0s 283us/step - loss: 0.0017\n",
      "Epoch 114/150\n",
      "34/34 [==============================] - 0s 270us/step - loss: 0.0016\n",
      "Epoch 115/150\n",
      "34/34 [==============================] - 0s 299us/step - loss: 0.0016\n",
      "Epoch 116/150\n",
      "34/34 [==============================] - 0s 300us/step - loss: 0.0016\n",
      "Epoch 117/150\n",
      "34/34 [==============================] - 0s 273us/step - loss: 0.0016\n",
      "Epoch 118/150\n",
      "34/34 [==============================] - 0s 286us/step - loss: 0.0016\n",
      "Epoch 119/150\n",
      "34/34 [==============================] - 0s 265us/step - loss: 0.0016\n",
      "Epoch 120/150\n",
      "34/34 [==============================] - 0s 282us/step - loss: 0.0016\n",
      "Epoch 121/150\n",
      "34/34 [==============================] - 0s 309us/step - loss: 0.0016\n",
      "Epoch 122/150\n",
      "34/34 [==============================] - 0s 358us/step - loss: 0.0016\n",
      "Epoch 123/150\n",
      "34/34 [==============================] - 0s 279us/step - loss: 0.0015\n",
      "Epoch 124/150\n",
      "34/34 [==============================] - 0s 281us/step - loss: 0.0015\n",
      "Epoch 125/150\n",
      "34/34 [==============================] - 0s 268us/step - loss: 0.0015\n",
      "Epoch 126/150\n",
      "34/34 [==============================] - 0s 301us/step - loss: 0.0015\n",
      "Epoch 127/150\n",
      "34/34 [==============================] - 0s 305us/step - loss: 0.0015\n",
      "Epoch 128/150\n",
      "34/34 [==============================] - 0s 253us/step - loss: 0.0015\n",
      "Epoch 129/150\n",
      "34/34 [==============================] - 0s 276us/step - loss: 0.0015\n",
      "Epoch 130/150\n",
      "34/34 [==============================] - 0s 277us/step - loss: 0.0015\n",
      "Epoch 131/150\n",
      "34/34 [==============================] - 0s 287us/step - loss: 0.0015\n",
      "Epoch 132/150\n",
      "34/34 [==============================] - 0s 307us/step - loss: 0.0015\n",
      "Epoch 133/150\n",
      "34/34 [==============================] - 0s 246us/step - loss: 0.0014\n",
      "Epoch 134/150\n",
      "34/34 [==============================] - 0s 296us/step - loss: 0.0014\n",
      "Epoch 135/150\n",
      "34/34 [==============================] - 0s 251us/step - loss: 0.0014\n",
      "Epoch 136/150\n",
      "34/34 [==============================] - 0s 245us/step - loss: 0.0014\n",
      "Epoch 137/150\n",
      "34/34 [==============================] - 0s 240us/step - loss: 0.0014\n",
      "Epoch 138/150\n",
      "34/34 [==============================] - 0s 267us/step - loss: 0.0014\n",
      "Epoch 139/150\n",
      "34/34 [==============================] - 0s 259us/step - loss: 0.0014\n",
      "Epoch 140/150\n",
      "34/34 [==============================] - 0s 269us/step - loss: 0.0014\n",
      "Epoch 141/150\n",
      "34/34 [==============================] - 0s 306us/step - loss: 0.0014\n",
      "Epoch 142/150\n",
      "34/34 [==============================] - 0s 255us/step - loss: 0.0014\n",
      "Epoch 143/150\n",
      "34/34 [==============================] - 0s 260us/step - loss: 0.0014\n",
      "Epoch 144/150\n",
      "34/34 [==============================] - 0s 272us/step - loss: 0.0014\n",
      "Epoch 145/150\n",
      "34/34 [==============================] - 0s 292us/step - loss: 0.0013\n",
      "Epoch 146/150\n",
      "34/34 [==============================] - 0s 345us/step - loss: 0.0013\n",
      "Epoch 147/150\n",
      "34/34 [==============================] - 0s 323us/step - loss: 0.0013\n",
      "Epoch 148/150\n",
      "34/34 [==============================] - 0s 360us/step - loss: 0.0013\n",
      "Epoch 149/150\n",
      "34/34 [==============================] - 0s 266us/step - loss: 0.0013\n",
      "Epoch 150/150\n",
      "34/34 [==============================] - 0s 261us/step - loss: 0.0013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4b83ab10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate test and training data splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=445)\n",
    "\n",
    "# Train the network\n",
    "baseNetwork = base_model()\n",
    "\n",
    "baseNetwork.fit(X_train, y_train, epochs=150, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the testing set: 0.0032009948045015335\n"
     ]
    }
   ],
   "source": [
    "test_loss = baseNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "print('The loss on the testing set:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss using sklearn method: 0.015236329999627063\n"
     ]
    }
   ],
   "source": [
    "# Compare to exact predictor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n",
    "pred_values = reg.predict(X_train)\n",
    "mse = np.sqrt( np.mean((pred_values - y_train) ** 2 ) )\n",
    "print('The loss using sklearn method:', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single hidden layer\n",
    "As a first pass at a more complex topology, I will use a network with a single hidden layer smaller than the input data (which is 17 dimensional). The hidden layer will arbitrarily be chosen to be of size 8.\n",
    "\n",
    "The motivation for this structure is as follows:\n",
    "1. The initial attempt using linear regression considered all parameters individually. However, the effectiveness of these measures are not independent of each other (eg. high testing rates will not do anything without some quarantine measures). \n",
    "2. The hidden layer might be able to capture in essence the idea that the effectiveness is dependent on several features together.\n",
    "3. The hidden layer could be expected to be smaller than the input since not all combinations are significant there are many combinations of parameters avaialble.\n",
    "\n",
    "I will use a sigmoid activation function in the hidden layer and again the mean squared error loss function. The sigmoid activation seems to be a decent choice since the hope is that the hidden layer will capture some essence of combinations of the features, so the normalizing effect should help the output neuron be less biased (not really sure if this is a valid argument). Additonally, the data is not too extreme in value and the dataset is relatively small so it shouldn't have too many convergence issues.\n",
    "\n",
    "As a first attempt, I will use the Adam optimizer because it is popular and I don't quite understand all the parameters available in SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thin_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=17, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    # Initialize weights\n",
    "    model.layers[0].set_weights([np.zeros([17,8])+0.01, np.zeros([8,])])\n",
    "    model.layers[1].set_weights([np.zeros([8,1])+0.01, np.array([0.00])])\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "34/34 [==============================] - 1s 19ms/step - loss: 0.0136\n",
      "Epoch 2/150\n",
      "34/34 [==============================] - 0s 276us/step - loss: 0.0037\n",
      "Epoch 3/150\n",
      "34/34 [==============================] - 0s 291us/step - loss: 0.0011\n",
      "Epoch 4/150\n",
      "34/34 [==============================] - 0s 326us/step - loss: 0.0016\n",
      "Epoch 5/150\n",
      "34/34 [==============================] - 0s 354us/step - loss: 0.0018\n",
      "Epoch 6/150\n",
      "34/34 [==============================] - 0s 344us/step - loss: 0.0014\n",
      "Epoch 7/150\n",
      "34/34 [==============================] - 0s 342us/step - loss: 0.0013\n",
      "Epoch 8/150\n",
      "34/34 [==============================] - 0s 362us/step - loss: 0.0012\n",
      "Epoch 9/150\n",
      "34/34 [==============================] - 0s 343us/step - loss: 0.0012\n",
      "Epoch 10/150\n",
      "34/34 [==============================] - 0s 510us/step - loss: 0.0012\n",
      "Epoch 11/150\n",
      "34/34 [==============================] - 0s 342us/step - loss: 0.0012\n",
      "Epoch 12/150\n",
      "34/34 [==============================] - 0s 400us/step - loss: 0.0012\n",
      "Epoch 13/150\n",
      "34/34 [==============================] - 0s 399us/step - loss: 0.0012\n",
      "Epoch 14/150\n",
      "34/34 [==============================] - 0s 391us/step - loss: 0.0012\n",
      "Epoch 15/150\n",
      "34/34 [==============================] - 0s 401us/step - loss: 0.0012\n",
      "Epoch 16/150\n",
      "34/34 [==============================] - 0s 372us/step - loss: 0.0012\n",
      "Epoch 17/150\n",
      "34/34 [==============================] - 0s 438us/step - loss: 0.0012\n",
      "Epoch 18/150\n",
      "34/34 [==============================] - 0s 512us/step - loss: 0.0012\n",
      "Epoch 19/150\n",
      "34/34 [==============================] - 0s 388us/step - loss: 0.0012\n",
      "Epoch 20/150\n",
      "34/34 [==============================] - 0s 374us/step - loss: 0.0011\n",
      "Epoch 21/150\n",
      "34/34 [==============================] - 0s 417us/step - loss: 0.0010\n",
      "Epoch 22/150\n",
      "34/34 [==============================] - 0s 419us/step - loss: 8.9310e-04\n",
      "Epoch 23/150\n",
      "34/34 [==============================] - 0s 399us/step - loss: 8.6763e-04\n",
      "Epoch 24/150\n",
      "34/34 [==============================] - 0s 372us/step - loss: 9.2809e-04\n",
      "Epoch 25/150\n",
      "34/34 [==============================] - 0s 380us/step - loss: 8.7521e-04\n",
      "Epoch 26/150\n",
      "34/34 [==============================] - 0s 356us/step - loss: 9.0216e-04\n",
      "Epoch 27/150\n",
      "34/34 [==============================] - 0s 468us/step - loss: 8.5527e-04\n",
      "Epoch 28/150\n",
      "34/34 [==============================] - 0s 427us/step - loss: 8.5633e-04\n",
      "Epoch 29/150\n",
      "34/34 [==============================] - 0s 388us/step - loss: 8.5978e-04\n",
      "Epoch 30/150\n",
      "34/34 [==============================] - 0s 382us/step - loss: 8.4671e-04\n",
      "Epoch 31/150\n",
      "34/34 [==============================] - 0s 389us/step - loss: 8.5163e-04\n",
      "Epoch 32/150\n",
      "34/34 [==============================] - 0s 400us/step - loss: 8.4574e-04\n",
      "Epoch 33/150\n",
      "34/34 [==============================] - 0s 363us/step - loss: 8.3885e-04\n",
      "Epoch 34/150\n",
      "34/34 [==============================] - 0s 376us/step - loss: 8.3737e-04\n",
      "Epoch 35/150\n",
      "34/34 [==============================] - 0s 459us/step - loss: 8.3189e-04\n",
      "Epoch 36/150\n",
      "34/34 [==============================] - 0s 484us/step - loss: 8.2825e-04\n",
      "Epoch 37/150\n",
      "34/34 [==============================] - 0s 440us/step - loss: 8.2515e-04\n",
      "Epoch 38/150\n",
      "34/34 [==============================] - 0s 439us/step - loss: 8.2032e-04\n",
      "Epoch 39/150\n",
      "34/34 [==============================] - 0s 372us/step - loss: 8.1634e-04\n",
      "Epoch 40/150\n",
      "34/34 [==============================] - 0s 385us/step - loss: 8.1198e-04\n",
      "Epoch 41/150\n",
      "34/34 [==============================] - 0s 405us/step - loss: 8.0735e-04\n",
      "Epoch 42/150\n",
      "34/34 [==============================] - 0s 388us/step - loss: 8.0318e-04\n",
      "Epoch 43/150\n",
      "34/34 [==============================] - 0s 371us/step - loss: 7.9885e-04\n",
      "Epoch 44/150\n",
      "34/34 [==============================] - 0s 409us/step - loss: 7.9486e-04\n",
      "Epoch 45/150\n",
      "34/34 [==============================] - 0s 377us/step - loss: 7.9122e-04\n",
      "Epoch 46/150\n",
      "34/34 [==============================] - 0s 388us/step - loss: 7.8784e-04\n",
      "Epoch 47/150\n",
      "34/34 [==============================] - 0s 425us/step - loss: 7.8485e-04\n",
      "Epoch 48/150\n",
      "34/34 [==============================] - 0s 411us/step - loss: 7.8207e-04\n",
      "Epoch 49/150\n",
      "34/34 [==============================] - 0s 400us/step - loss: 7.7950e-04\n",
      "Epoch 50/150\n",
      "34/34 [==============================] - 0s 382us/step - loss: 7.7711e-04\n",
      "Epoch 51/150\n",
      "34/34 [==============================] - 0s 358us/step - loss: 7.7487e-04\n",
      "Epoch 52/150\n",
      "34/34 [==============================] - 0s 412us/step - loss: 7.7278e-04\n",
      "Epoch 53/150\n",
      "34/34 [==============================] - 0s 393us/step - loss: 7.7081e-04\n",
      "Epoch 54/150\n",
      "34/34 [==============================] - 0s 365us/step - loss: 7.6897e-04\n",
      "Epoch 55/150\n",
      "34/34 [==============================] - 0s 403us/step - loss: 7.6724e-04\n",
      "Epoch 56/150\n",
      "34/34 [==============================] - 0s 405us/step - loss: 7.6560e-04\n",
      "Epoch 57/150\n",
      "34/34 [==============================] - 0s 359us/step - loss: 7.6404e-04\n",
      "Epoch 58/150\n",
      "34/34 [==============================] - 0s 393us/step - loss: 7.6254e-04\n",
      "Epoch 59/150\n",
      "34/34 [==============================] - 0s 325us/step - loss: 7.6111e-04\n",
      "Epoch 60/150\n",
      "34/34 [==============================] - 0s 330us/step - loss: 7.5972e-04\n",
      "Epoch 61/150\n",
      "34/34 [==============================] - 0s 312us/step - loss: 7.5839e-04\n",
      "Epoch 62/150\n",
      "34/34 [==============================] - 0s 338us/step - loss: 7.5709e-04\n",
      "Epoch 63/150\n",
      "34/34 [==============================] - 0s 328us/step - loss: 7.5582e-04\n",
      "Epoch 64/150\n",
      "34/34 [==============================] - 0s 323us/step - loss: 7.5459e-04\n",
      "Epoch 65/150\n",
      "34/34 [==============================] - 0s 306us/step - loss: 7.5338e-04\n",
      "Epoch 66/150\n",
      "34/34 [==============================] - 0s 366us/step - loss: 7.5220e-04\n",
      "Epoch 67/150\n",
      "34/34 [==============================] - 0s 362us/step - loss: 7.5105e-04\n",
      "Epoch 68/150\n",
      "34/34 [==============================] - 0s 347us/step - loss: 7.4991e-04\n",
      "Epoch 69/150\n",
      "34/34 [==============================] - 0s 300us/step - loss: 7.4880e-04\n",
      "Epoch 70/150\n",
      "34/34 [==============================] - 0s 284us/step - loss: 7.4770e-04\n",
      "Epoch 71/150\n",
      "34/34 [==============================] - 0s 315us/step - loss: 7.4662e-04\n",
      "Epoch 72/150\n",
      "34/34 [==============================] - 0s 292us/step - loss: 7.4555e-04\n",
      "Epoch 73/150\n",
      "34/34 [==============================] - 0s 339us/step - loss: 7.4450e-04\n",
      "Epoch 74/150\n",
      "34/34 [==============================] - 0s 333us/step - loss: 7.4346e-04\n",
      "Epoch 75/150\n",
      "34/34 [==============================] - 0s 297us/step - loss: 7.4244e-04\n",
      "Epoch 76/150\n",
      "34/34 [==============================] - 0s 326us/step - loss: 7.4143e-04\n",
      "Epoch 77/150\n",
      "34/34 [==============================] - 0s 302us/step - loss: 7.4042e-04\n",
      "Epoch 78/150\n",
      "34/34 [==============================] - 0s 310us/step - loss: 7.3943e-04\n",
      "Epoch 79/150\n",
      "34/34 [==============================] - 0s 374us/step - loss: 7.3844e-04\n",
      "Epoch 80/150\n",
      "34/34 [==============================] - 0s 335us/step - loss: 7.3750e-04\n",
      "Epoch 81/150\n",
      "34/34 [==============================] - 0s 298us/step - loss: 7.3649e-04\n",
      "Epoch 82/150\n",
      "34/34 [==============================] - 0s 293us/step - loss: 7.3557e-04\n",
      "Epoch 83/150\n",
      "34/34 [==============================] - 0s 324us/step - loss: 7.3461e-04\n",
      "Epoch 84/150\n",
      "34/34 [==============================] - 0s 295us/step - loss: 7.3367e-04\n",
      "Epoch 85/150\n",
      "34/34 [==============================] - 0s 312us/step - loss: 7.3275e-04\n",
      "Epoch 86/150\n",
      "34/34 [==============================] - 0s 341us/step - loss: 7.3182e-04\n",
      "Epoch 87/150\n",
      "34/34 [==============================] - 0s 302us/step - loss: 7.3090e-04\n",
      "Epoch 88/150\n",
      "34/34 [==============================] - 0s 292us/step - loss: 7.3000e-04\n",
      "Epoch 89/150\n",
      "34/34 [==============================] - 0s 373us/step - loss: 7.2909e-04\n",
      "Epoch 90/150\n",
      "34/34 [==============================] - 0s 376us/step - loss: 7.2820e-04\n",
      "Epoch 91/150\n",
      "34/34 [==============================] - 0s 385us/step - loss: 7.2730e-04\n",
      "Epoch 92/150\n",
      "34/34 [==============================] - 0s 325us/step - loss: 7.2642e-04\n",
      "Epoch 93/150\n",
      "34/34 [==============================] - 0s 325us/step - loss: 7.2554e-04\n",
      "Epoch 94/150\n",
      "34/34 [==============================] - 0s 322us/step - loss: 7.2467e-04\n",
      "Epoch 95/150\n",
      "34/34 [==============================] - 0s 318us/step - loss: 7.2380e-04\n",
      "Epoch 96/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 338us/step - loss: 7.2293e-04\n",
      "Epoch 97/150\n",
      "34/34 [==============================] - 0s 353us/step - loss: 7.2216e-04\n",
      "Epoch 98/150\n",
      "34/34 [==============================] - 0s 311us/step - loss: 7.2112e-04\n",
      "Epoch 99/150\n",
      "34/34 [==============================] - 0s 313us/step - loss: 7.2044e-04\n",
      "Epoch 100/150\n",
      "34/34 [==============================] - 0s 322us/step - loss: 7.1951e-04\n",
      "Epoch 101/150\n",
      "34/34 [==============================] - 0s 360us/step - loss: 7.1870e-04\n",
      "Epoch 102/150\n",
      "34/34 [==============================] - 0s 333us/step - loss: 7.1788e-04\n",
      "Epoch 103/150\n",
      "34/34 [==============================] - 0s 300us/step - loss: 7.1702e-04\n",
      "Epoch 104/150\n",
      "34/34 [==============================] - 0s 273us/step - loss: 7.1622e-04\n",
      "Epoch 105/150\n",
      "34/34 [==============================] - 0s 300us/step - loss: 7.1539e-04\n",
      "Epoch 106/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 7.1458e-04\n",
      "Epoch 107/150\n",
      "34/34 [==============================] - 0s 312us/step - loss: 7.1377e-04\n",
      "Epoch 108/150\n",
      "34/34 [==============================] - 0s 298us/step - loss: 7.1295e-04\n",
      "Epoch 109/150\n",
      "34/34 [==============================] - 0s 318us/step - loss: 7.1215e-04\n",
      "Epoch 110/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 7.1135e-04\n",
      "Epoch 111/150\n",
      "34/34 [==============================] - 0s 279us/step - loss: 7.1055e-04\n",
      "Epoch 112/150\n",
      "34/34 [==============================] - 0s 376us/step - loss: 7.0976e-04\n",
      "Epoch 113/150\n",
      "34/34 [==============================] - 0s 372us/step - loss: 7.0897e-04\n",
      "Epoch 114/150\n",
      "34/34 [==============================] - 0s 326us/step - loss: 7.0819e-04\n",
      "Epoch 115/150\n",
      "34/34 [==============================] - 0s 316us/step - loss: 7.0740e-04\n",
      "Epoch 116/150\n",
      "34/34 [==============================] - 0s 293us/step - loss: 7.0662e-04\n",
      "Epoch 117/150\n",
      "34/34 [==============================] - 0s 310us/step - loss: 7.0585e-04\n",
      "Epoch 118/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 7.0507e-04\n",
      "Epoch 119/150\n",
      "34/34 [==============================] - 0s 310us/step - loss: 7.0430e-04\n",
      "Epoch 120/150\n",
      "34/34 [==============================] - 0s 307us/step - loss: 7.0354e-04\n",
      "Epoch 121/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 7.0277e-04\n",
      "Epoch 122/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 7.0201e-04\n",
      "Epoch 123/150\n",
      "34/34 [==============================] - 0s 282us/step - loss: 7.0125e-04\n",
      "Epoch 124/150\n",
      "34/34 [==============================] - 0s 284us/step - loss: 7.0050e-04\n",
      "Epoch 125/150\n",
      "34/34 [==============================] - 0s 321us/step - loss: 6.9975e-04\n",
      "Epoch 126/150\n",
      "34/34 [==============================] - 0s 316us/step - loss: 6.9900e-04\n",
      "Epoch 127/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 6.9825e-04\n",
      "Epoch 128/150\n",
      "34/34 [==============================] - 0s 292us/step - loss: 6.9751e-04\n",
      "Epoch 129/150\n",
      "34/34 [==============================] - 0s 315us/step - loss: 6.9676e-04\n",
      "Epoch 130/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 6.9602e-04\n",
      "Epoch 131/150\n",
      "34/34 [==============================] - 0s 314us/step - loss: 6.9529e-04\n",
      "Epoch 132/150\n",
      "34/34 [==============================] - 0s 285us/step - loss: 6.9455e-04\n",
      "Epoch 133/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 6.9382e-04\n",
      "Epoch 134/150\n",
      "34/34 [==============================] - 0s 277us/step - loss: 6.9309e-04\n",
      "Epoch 135/150\n",
      "34/34 [==============================] - 0s 266us/step - loss: 6.9237e-04\n",
      "Epoch 136/150\n",
      "34/34 [==============================] - 0s 341us/step - loss: 6.9164e-04\n",
      "Epoch 137/150\n",
      "34/34 [==============================] - 0s 356us/step - loss: 6.9092e-04\n",
      "Epoch 138/150\n",
      "34/34 [==============================] - 0s 332us/step - loss: 6.9020e-04\n",
      "Epoch 139/150\n",
      "34/34 [==============================] - 0s 303us/step - loss: 6.8949e-04\n",
      "Epoch 140/150\n",
      "34/34 [==============================] - 0s 326us/step - loss: 6.8877e-04\n",
      "Epoch 141/150\n",
      "34/34 [==============================] - 0s 308us/step - loss: 6.8806e-04\n",
      "Epoch 142/150\n",
      "34/34 [==============================] - 0s 284us/step - loss: 6.8735e-04\n",
      "Epoch 143/150\n",
      "34/34 [==============================] - 0s 362us/step - loss: 6.8664e-04\n",
      "Epoch 144/150\n",
      "34/34 [==============================] - 0s 341us/step - loss: 6.8593e-04\n",
      "Epoch 145/150\n",
      "34/34 [==============================] - 0s 313us/step - loss: 6.8523e-04\n",
      "Epoch 146/150\n",
      "34/34 [==============================] - 0s 291us/step - loss: 6.8453e-04\n",
      "Epoch 147/150\n",
      "34/34 [==============================] - 0s 327us/step - loss: 6.8383e-04\n",
      "Epoch 148/150\n",
      "34/34 [==============================] - 0s 284us/step - loss: 6.8313e-04\n",
      "Epoch 149/150\n",
      "34/34 [==============================] - 0s 343us/step - loss: 6.8243e-04\n",
      "Epoch 150/150\n",
      "34/34 [==============================] - 0s 312us/step - loss: 6.8174e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4bb7acd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thinNetwork = thin_model()\n",
    "\n",
    "\n",
    "\n",
    "thinNetwork.fit(X_train, y_train, epochs=150, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the testing set: 0.0014621701557189226\n"
     ]
    }
   ],
   "source": [
    "test_loss = thinNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "print('The loss on the testing set:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, at least for this particular training and testing set, the single layer thing network did a significantly better job on predicting the output than the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two hidden layers\n",
    "Adding additional complexity, I will add a second hidden layer, also of size 8.\n",
    "\n",
    "The hope was that the first hidden layer in the previous attempt might capture the essence of \"combinations\" of the parameters being significant. However, not all combinations are trivial. For instance, high early testing rates might be significant only if quarantine measures are enacted, but quarantine measures could be enacted without high testing rates and still be effective. Additional complexities like this might hopefully be captured by the addition of a second hidden layer.\n",
    "\n",
    "I will use the same parameters for the second layer as well (at least initially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=17, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(8, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "34/34 [==============================] - 1s 25ms/step - loss: 0.0262\n",
      "Epoch 2/150\n",
      "34/34 [==============================] - 0s 343us/step - loss: 0.0156\n",
      "Epoch 3/150\n",
      "34/34 [==============================] - 0s 382us/step - loss: 0.0079\n",
      "Epoch 4/150\n",
      "34/34 [==============================] - 0s 373us/step - loss: 0.0033\n",
      "Epoch 5/150\n",
      "34/34 [==============================] - 0s 412us/step - loss: 0.0014\n",
      "Epoch 6/150\n",
      "34/34 [==============================] - 0s 412us/step - loss: 0.0011\n",
      "Epoch 7/150\n",
      "34/34 [==============================] - 0s 402us/step - loss: 0.0013\n",
      "Epoch 8/150\n",
      "34/34 [==============================] - 0s 438us/step - loss: 0.0014\n",
      "Epoch 9/150\n",
      "34/34 [==============================] - 0s 438us/step - loss: 0.0013\n",
      "Epoch 10/150\n",
      "34/34 [==============================] - 0s 442us/step - loss: 0.0012\n",
      "Epoch 11/150\n",
      "34/34 [==============================] - 0s 438us/step - loss: 0.0012\n",
      "Epoch 12/150\n",
      "34/34 [==============================] - 0s 434us/step - loss: 0.0012\n",
      "Epoch 13/150\n",
      "34/34 [==============================] - 0s 527us/step - loss: 0.0012\n",
      "Epoch 14/150\n",
      "34/34 [==============================] - 0s 506us/step - loss: 0.0012\n",
      "Epoch 15/150\n",
      "34/34 [==============================] - 0s 364us/step - loss: 0.0012\n",
      "Epoch 16/150\n",
      "34/34 [==============================] - 0s 387us/step - loss: 0.0012\n",
      "Epoch 17/150\n",
      "34/34 [==============================] - 0s 406us/step - loss: 0.0012\n",
      "Epoch 18/150\n",
      "34/34 [==============================] - 0s 394us/step - loss: 0.0012\n",
      "Epoch 19/150\n",
      "34/34 [==============================] - 0s 395us/step - loss: 0.0012\n",
      "Epoch 20/150\n",
      "34/34 [==============================] - 0s 380us/step - loss: 0.0012\n",
      "Epoch 21/150\n",
      "34/34 [==============================] - 0s 449us/step - loss: 0.0012\n",
      "Epoch 22/150\n",
      "34/34 [==============================] - 0s 517us/step - loss: 0.0012\n",
      "Epoch 23/150\n",
      "34/34 [==============================] - 0s 459us/step - loss: 0.0012\n",
      "Epoch 24/150\n",
      "34/34 [==============================] - 0s 397us/step - loss: 0.0012\n",
      "Epoch 25/150\n",
      "34/34 [==============================] - 0s 440us/step - loss: 0.0011\n",
      "Epoch 26/150\n",
      "34/34 [==============================] - 0s 421us/step - loss: 9.7099e-04\n",
      "Epoch 27/150\n",
      "34/34 [==============================] - 0s 409us/step - loss: 9.2988e-04\n",
      "Epoch 28/150\n",
      "34/34 [==============================] - 0s 406us/step - loss: 9.3615e-04\n",
      "Epoch 29/150\n",
      "34/34 [==============================] - 0s 406us/step - loss: 9.3019e-04\n",
      "Epoch 30/150\n",
      "34/34 [==============================] - 0s 430us/step - loss: 9.1573e-04\n",
      "Epoch 31/150\n",
      "34/34 [==============================] - 0s 436us/step - loss: 9.0678e-04\n",
      "Epoch 32/150\n",
      "34/34 [==============================] - 0s 466us/step - loss: 9.0408e-04\n",
      "Epoch 33/150\n",
      "34/34 [==============================] - 0s 461us/step - loss: 9.0161e-04\n",
      "Epoch 34/150\n",
      "34/34 [==============================] - 0s 479us/step - loss: 8.9722e-04\n",
      "Epoch 35/150\n",
      "34/34 [==============================] - 0s 419us/step - loss: 8.9219e-04\n",
      "Epoch 36/150\n",
      "34/34 [==============================] - 0s 655us/step - loss: 8.8793e-04\n",
      "Epoch 37/150\n",
      "34/34 [==============================] - 0s 431us/step - loss: 8.8461e-04\n",
      "Epoch 38/150\n",
      "34/34 [==============================] - 0s 406us/step - loss: 8.8168e-04\n",
      "Epoch 39/150\n",
      "34/34 [==============================] - 0s 403us/step - loss: 8.7871e-04\n",
      "Epoch 40/150\n",
      "34/34 [==============================] - 0s 394us/step - loss: 8.7573e-04\n",
      "Epoch 41/150\n",
      "34/34 [==============================] - 0s 414us/step - loss: 8.7289e-04\n",
      "Epoch 42/150\n",
      "34/34 [==============================] - 0s 399us/step - loss: 8.7028e-04\n",
      "Epoch 43/150\n",
      "34/34 [==============================] - 0s 444us/step - loss: 8.6783e-04\n",
      "Epoch 44/150\n",
      "34/34 [==============================] - 0s 513us/step - loss: 8.6548e-04\n",
      "Epoch 45/150\n",
      "34/34 [==============================] - 0s 451us/step - loss: 8.6322e-04\n",
      "Epoch 46/150\n",
      "34/34 [==============================] - 0s 430us/step - loss: 8.6104e-04\n",
      "Epoch 47/150\n",
      "34/34 [==============================] - 0s 459us/step - loss: 8.5899e-04\n",
      "Epoch 48/150\n",
      "34/34 [==============================] - 0s 440us/step - loss: 8.5704e-04\n",
      "Epoch 49/150\n",
      "34/34 [==============================] - 0s 445us/step - loss: 8.5519e-04\n",
      "Epoch 50/150\n",
      "34/34 [==============================] - 0s 395us/step - loss: 8.5342e-04\n",
      "Epoch 51/150\n",
      "34/34 [==============================] - 0s 378us/step - loss: 8.5174e-04\n",
      "Epoch 52/150\n",
      "34/34 [==============================] - 0s 451us/step - loss: 8.5013e-04\n",
      "Epoch 53/150\n",
      "34/34 [==============================] - 0s 608us/step - loss: 8.4861e-04\n",
      "Epoch 54/150\n",
      "34/34 [==============================] - 0s 421us/step - loss: 8.4716e-04\n",
      "Epoch 55/150\n",
      "34/34 [==============================] - 0s 412us/step - loss: 8.4578e-04\n",
      "Epoch 56/150\n",
      "34/34 [==============================] - 0s 427us/step - loss: 8.4446e-04\n",
      "Epoch 57/150\n",
      "34/34 [==============================] - 0s 379us/step - loss: 8.4321e-04\n",
      "Epoch 58/150\n",
      "34/34 [==============================] - 0s 365us/step - loss: 8.4202e-04\n",
      "Epoch 59/150\n",
      "34/34 [==============================] - 0s 339us/step - loss: 8.4089e-04\n",
      "Epoch 60/150\n",
      "34/34 [==============================] - 0s 372us/step - loss: 8.3981e-04\n",
      "Epoch 61/150\n",
      "34/34 [==============================] - 0s 420us/step - loss: 8.3879e-04\n",
      "Epoch 62/150\n",
      "34/34 [==============================] - 0s 349us/step - loss: 8.3781e-04\n",
      "Epoch 63/150\n",
      "34/34 [==============================] - 0s 352us/step - loss: 8.3689e-04\n",
      "Epoch 64/150\n",
      "34/34 [==============================] - 0s 342us/step - loss: 8.3600e-04\n",
      "Epoch 65/150\n",
      "34/34 [==============================] - 0s 378us/step - loss: 8.3516e-04\n",
      "Epoch 66/150\n",
      "34/34 [==============================] - 0s 349us/step - loss: 8.3436e-04\n",
      "Epoch 67/150\n",
      "34/34 [==============================] - 0s 356us/step - loss: 8.3360e-04\n",
      "Epoch 68/150\n",
      "34/34 [==============================] - 0s 386us/step - loss: 8.3288e-04\n",
      "Epoch 69/150\n",
      "34/34 [==============================] - 0s 329us/step - loss: 8.3219e-04\n",
      "Epoch 70/150\n",
      "34/34 [==============================] - 0s 366us/step - loss: 8.3153e-04\n",
      "Epoch 71/150\n",
      "34/34 [==============================] - 0s 361us/step - loss: 8.3090e-04\n",
      "Epoch 72/150\n",
      "34/34 [==============================] - 0s 486us/step - loss: 8.3030e-04\n",
      "Epoch 73/150\n",
      "34/34 [==============================] - 0s 373us/step - loss: 8.2973e-04\n",
      "Epoch 74/150\n",
      "34/34 [==============================] - 0s 355us/step - loss: 8.2919e-04\n",
      "Epoch 75/150\n",
      "34/34 [==============================] - 0s 390us/step - loss: 8.2867e-04\n",
      "Epoch 76/150\n",
      "34/34 [==============================] - 0s 375us/step - loss: 8.2818e-04\n",
      "Epoch 77/150\n",
      "34/34 [==============================] - 0s 353us/step - loss: 8.2770e-04\n",
      "Epoch 78/150\n",
      "34/34 [==============================] - 0s 345us/step - loss: 8.2725e-04\n",
      "Epoch 79/150\n",
      "34/34 [==============================] - 0s 341us/step - loss: 8.2682e-04\n",
      "Epoch 80/150\n",
      "34/34 [==============================] - 0s 400us/step - loss: 8.2641e-04\n",
      "Epoch 81/150\n",
      "34/34 [==============================] - 0s 409us/step - loss: 8.2601e-04\n",
      "Epoch 82/150\n",
      "34/34 [==============================] - 0s 358us/step - loss: 8.2563e-04\n",
      "Epoch 83/150\n",
      "34/34 [==============================] - 0s 345us/step - loss: 8.2527e-04\n",
      "Epoch 84/150\n",
      "34/34 [==============================] - 0s 360us/step - loss: 8.2492e-04\n",
      "Epoch 85/150\n",
      "34/34 [==============================] - 0s 378us/step - loss: 8.2458e-04\n",
      "Epoch 86/150\n",
      "34/34 [==============================] - 0s 371us/step - loss: 8.2426e-04\n",
      "Epoch 87/150\n",
      "34/34 [==============================] - 0s 346us/step - loss: 8.2395e-04\n",
      "Epoch 88/150\n",
      "34/34 [==============================] - 0s 390us/step - loss: 8.2366e-04\n",
      "Epoch 89/150\n",
      "34/34 [==============================] - 0s 352us/step - loss: 8.2337e-04\n",
      "Epoch 90/150\n",
      "34/34 [==============================] - 0s 367us/step - loss: 8.2309e-04\n",
      "Epoch 91/150\n",
      "34/34 [==============================] - 0s 338us/step - loss: 8.2283e-04\n",
      "Epoch 92/150\n",
      "34/34 [==============================] - 0s 361us/step - loss: 8.2257e-04\n",
      "Epoch 93/150\n",
      "34/34 [==============================] - 0s 381us/step - loss: 8.2233e-04\n",
      "Epoch 94/150\n",
      "34/34 [==============================] - 0s 515us/step - loss: 8.2209e-04\n",
      "Epoch 95/150\n",
      "34/34 [==============================] - 0s 403us/step - loss: 8.2186e-04\n",
      "Epoch 96/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 317us/step - loss: 8.2163e-04\n",
      "Epoch 97/150\n",
      "34/34 [==============================] - 0s 369us/step - loss: 8.2142e-04\n",
      "Epoch 98/150\n",
      "34/34 [==============================] - 0s 333us/step - loss: 8.2121e-04\n",
      "Epoch 99/150\n",
      "34/34 [==============================] - 0s 417us/step - loss: 8.2100e-04\n",
      "Epoch 100/150\n",
      "34/34 [==============================] - 0s 392us/step - loss: 8.2080e-04\n",
      "Epoch 101/150\n",
      "34/34 [==============================] - 0s 326us/step - loss: 8.2061e-04\n",
      "Epoch 102/150\n",
      "34/34 [==============================] - 0s 301us/step - loss: 8.2043e-04\n",
      "Epoch 103/150\n",
      "34/34 [==============================] - 0s 389us/step - loss: 8.2024e-04\n",
      "Epoch 104/150\n",
      "34/34 [==============================] - 0s 438us/step - loss: 8.2007e-04\n",
      "Epoch 105/150\n",
      "34/34 [==============================] - 0s 390us/step - loss: 8.1989e-04\n",
      "Epoch 106/150\n",
      "34/34 [==============================] - 0s 314us/step - loss: 8.1973e-04\n",
      "Epoch 107/150\n",
      "34/34 [==============================] - 0s 317us/step - loss: 8.1956e-04\n",
      "Epoch 108/150\n",
      "34/34 [==============================] - 0s 348us/step - loss: 8.1940e-04\n",
      "Epoch 109/150\n",
      "34/34 [==============================] - 0s 364us/step - loss: 8.1925e-04\n",
      "Epoch 110/150\n",
      "34/34 [==============================] - 0s 354us/step - loss: 8.1910e-04\n",
      "Epoch 111/150\n",
      "34/34 [==============================] - 0s 317us/step - loss: 8.1895e-04\n",
      "Epoch 112/150\n",
      "34/34 [==============================] - 0s 315us/step - loss: 8.1880e-04\n",
      "Epoch 113/150\n",
      "34/34 [==============================] - 0s 361us/step - loss: 8.1866e-04\n",
      "Epoch 114/150\n",
      "34/34 [==============================] - 0s 395us/step - loss: 8.1852e-04\n",
      "Epoch 115/150\n",
      "34/34 [==============================] - 0s 430us/step - loss: 8.1838e-04\n",
      "Epoch 116/150\n",
      "34/34 [==============================] - 0s 333us/step - loss: 8.1825e-04\n",
      "Epoch 117/150\n",
      "34/34 [==============================] - 0s 328us/step - loss: 8.1811e-04\n",
      "Epoch 118/150\n",
      "34/34 [==============================] - 0s 366us/step - loss: 8.1798e-04\n",
      "Epoch 119/150\n",
      "34/34 [==============================] - 0s 353us/step - loss: 8.1786e-04\n",
      "Epoch 120/150\n",
      "34/34 [==============================] - 0s 338us/step - loss: 8.1773e-04\n",
      "Epoch 121/150\n",
      "34/34 [==============================] - 0s 330us/step - loss: 8.1761e-04\n",
      "Epoch 122/150\n",
      "34/34 [==============================] - 0s 337us/step - loss: 8.1749e-04\n",
      "Epoch 123/150\n",
      "34/34 [==============================] - 0s 372us/step - loss: 8.1737e-04\n",
      "Epoch 124/150\n",
      "34/34 [==============================] - 0s 410us/step - loss: 8.1725e-04\n",
      "Epoch 125/150\n",
      "34/34 [==============================] - 0s 500us/step - loss: 8.1714e-04\n",
      "Epoch 126/150\n",
      "34/34 [==============================] - 0s 916us/step - loss: 8.1703e-04\n",
      "Epoch 127/150\n",
      "34/34 [==============================] - 0s 696us/step - loss: 8.1691e-04\n",
      "Epoch 128/150\n",
      "34/34 [==============================] - 0s 435us/step - loss: 8.1681e-04\n",
      "Epoch 129/150\n",
      "34/34 [==============================] - 0s 453us/step - loss: 8.1670e-04\n",
      "Epoch 130/150\n",
      "34/34 [==============================] - 0s 443us/step - loss: 8.1659e-04\n",
      "Epoch 131/150\n",
      "34/34 [==============================] - 0s 495us/step - loss: 8.1649e-04\n",
      "Epoch 132/150\n",
      "34/34 [==============================] - 0s 410us/step - loss: 8.1639e-04\n",
      "Epoch 133/150\n",
      "34/34 [==============================] - 0s 351us/step - loss: 8.1629e-04\n",
      "Epoch 134/150\n",
      "34/34 [==============================] - 0s 337us/step - loss: 8.1619e-04\n",
      "Epoch 135/150\n",
      "34/34 [==============================] - 0s 343us/step - loss: 8.1609e-04\n",
      "Epoch 136/150\n",
      "34/34 [==============================] - 0s 362us/step - loss: 8.1599e-04\n",
      "Epoch 137/150\n",
      "34/34 [==============================] - 0s 340us/step - loss: 8.1589e-04\n",
      "Epoch 138/150\n",
      "34/34 [==============================] - 0s 340us/step - loss: 8.1580e-04\n",
      "Epoch 139/150\n",
      "34/34 [==============================] - 0s 300us/step - loss: 8.1571e-04\n",
      "Epoch 140/150\n",
      "34/34 [==============================] - 0s 361us/step - loss: 8.1561e-04\n",
      "Epoch 141/150\n",
      "34/34 [==============================] - 0s 401us/step - loss: 8.1552e-04\n",
      "Epoch 142/150\n",
      "34/34 [==============================] - 0s 367us/step - loss: 8.1543e-04\n",
      "Epoch 143/150\n",
      "34/34 [==============================] - 0s 311us/step - loss: 8.1534e-04\n",
      "Epoch 144/150\n",
      "34/34 [==============================] - 0s 300us/step - loss: 8.1526e-04\n",
      "Epoch 145/150\n",
      "34/34 [==============================] - 0s 333us/step - loss: 8.1517e-04\n",
      "Epoch 146/150\n",
      "34/34 [==============================] - 0s 307us/step - loss: 8.1509e-04\n",
      "Epoch 147/150\n",
      "34/34 [==============================] - 0s 352us/step - loss: 8.1500e-04\n",
      "Epoch 148/150\n",
      "34/34 [==============================] - 0s 350us/step - loss: 8.1492e-04\n",
      "Epoch 149/150\n",
      "34/34 [==============================] - 0s 328us/step - loss: 8.1484e-04\n",
      "Epoch 150/150\n",
      "34/34 [==============================] - 0s 305us/step - loss: 8.1476e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4c414d90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepNetwork = deep_model()\n",
    "\n",
    "# Initialize weights\n",
    "deepNetwork.layers[0].set_weights([np.zeros([17,8])+0.01, np.zeros([8,])])\n",
    "deepNetwork.layers[1].set_weights([np.zeros([8,8])+0.01, np.zeros([8,])])\n",
    "deepNetwork.layers[2].set_weights([np.zeros([8,1])+0.01, np.array([0.00])])\n",
    "\n",
    "deepNetwork.fit(X_train, y_train, epochs=150, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the testing set: 0.0012646723771467805\n"
     ]
    }
   ],
   "source": [
    "test_loss = deepNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "print('The loss on the testing set:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, the additonal layer did not significantly improve the loss when a signmoid activation function is implemented. Instead trying a RELU activation in the second layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 0.0423\n",
      "Epoch 2/150\n",
      "34/34 [==============================] - 0s 450us/step - loss: 0.0336\n",
      "Epoch 3/150\n",
      "34/34 [==============================] - 0s 392us/step - loss: 0.0234\n",
      "Epoch 4/150\n",
      "34/34 [==============================] - 0s 379us/step - loss: 0.0132\n",
      "Epoch 5/150\n",
      "34/34 [==============================] - 0s 368us/step - loss: 0.0053\n",
      "Epoch 6/150\n",
      "34/34 [==============================] - 0s 401us/step - loss: 0.0015\n",
      "Epoch 7/150\n",
      "34/34 [==============================] - 0s 564us/step - loss: 0.0013\n",
      "Epoch 8/150\n",
      "34/34 [==============================] - 0s 359us/step - loss: 0.0018\n",
      "Epoch 9/150\n",
      "34/34 [==============================] - 0s 394us/step - loss: 0.0016\n",
      "Epoch 10/150\n",
      "34/34 [==============================] - 0s 393us/step - loss: 0.0013\n",
      "Epoch 11/150\n",
      "34/34 [==============================] - 0s 407us/step - loss: 0.0013\n",
      "Epoch 12/150\n",
      "34/34 [==============================] - 0s 399us/step - loss: 0.0012\n",
      "Epoch 13/150\n",
      "34/34 [==============================] - 0s 376us/step - loss: 0.0012\n",
      "Epoch 14/150\n",
      "34/34 [==============================] - 0s 432us/step - loss: 0.0012\n",
      "Epoch 15/150\n",
      "34/34 [==============================] - 0s 436us/step - loss: 0.0012\n",
      "Epoch 16/150\n",
      "34/34 [==============================] - 0s 404us/step - loss: 0.0012\n",
      "Epoch 17/150\n",
      "34/34 [==============================] - 0s 391us/step - loss: 0.0012\n",
      "Epoch 18/150\n",
      "34/34 [==============================] - 0s 427us/step - loss: 0.0012\n",
      "Epoch 19/150\n",
      "34/34 [==============================] - 0s 426us/step - loss: 0.0012\n",
      "Epoch 20/150\n",
      "34/34 [==============================] - 0s 425us/step - loss: 0.0012\n",
      "Epoch 21/150\n",
      "34/34 [==============================] - 0s 428us/step - loss: 0.0012\n",
      "Epoch 22/150\n",
      "34/34 [==============================] - 0s 437us/step - loss: 0.0012\n",
      "Epoch 23/150\n",
      "34/34 [==============================] - 0s 489us/step - loss: 0.0012\n",
      "Epoch 24/150\n",
      "34/34 [==============================] - 0s 502us/step - loss: 0.0012\n",
      "Epoch 25/150\n",
      "34/34 [==============================] - 0s 446us/step - loss: 0.0012\n",
      "Epoch 26/150\n",
      "34/34 [==============================] - 0s 458us/step - loss: 0.0012\n",
      "Epoch 27/150\n",
      "34/34 [==============================] - 0s 459us/step - loss: 0.0012\n",
      "Epoch 28/150\n",
      "34/34 [==============================] - 0s 473us/step - loss: 0.0012\n",
      "Epoch 29/150\n",
      "34/34 [==============================] - 0s 438us/step - loss: 0.0012\n",
      "Epoch 30/150\n",
      "34/34 [==============================] - 0s 430us/step - loss: 0.0012\n",
      "Epoch 31/150\n",
      "34/34 [==============================] - 0s 458us/step - loss: 0.0011\n",
      "Epoch 32/150\n",
      "34/34 [==============================] - 0s 494us/step - loss: 0.0011\n",
      "Epoch 33/150\n",
      "34/34 [==============================] - 0s 451us/step - loss: 9.3236e-04\n",
      "Epoch 34/150\n",
      "34/34 [==============================] - 0s 415us/step - loss: 9.7832e-04\n",
      "Epoch 35/150\n",
      "34/34 [==============================] - 0s 429us/step - loss: 9.3192e-04\n",
      "Epoch 36/150\n",
      "34/34 [==============================] - 0s 472us/step - loss: 8.5007e-04\n",
      "Epoch 37/150\n",
      "34/34 [==============================] - 0s 454us/step - loss: 9.1336e-04\n",
      "Epoch 38/150\n",
      "34/34 [==============================] - 0s 497us/step - loss: 8.7940e-04\n",
      "Epoch 39/150\n",
      "34/34 [==============================] - 0s 582us/step - loss: 8.5048e-04\n",
      "Epoch 40/150\n",
      "34/34 [==============================] - 0s 589us/step - loss: 8.7319e-04\n",
      "Epoch 41/150\n",
      "34/34 [==============================] - 0s 456us/step - loss: 8.5699e-04\n",
      "Epoch 42/150\n",
      "34/34 [==============================] - 0s 435us/step - loss: 8.4773e-04\n",
      "Epoch 43/150\n",
      "34/34 [==============================] - 0s 486us/step - loss: 8.5397e-04\n",
      "Epoch 44/150\n",
      "34/34 [==============================] - 0s 439us/step - loss: 8.4483e-04\n",
      "Epoch 45/150\n",
      "34/34 [==============================] - 0s 436us/step - loss: 8.4053e-04\n",
      "Epoch 46/150\n",
      "34/34 [==============================] - 0s 438us/step - loss: 8.4001e-04\n",
      "Epoch 47/150\n",
      "34/34 [==============================] - 0s 453us/step - loss: 8.3400e-04\n",
      "Epoch 48/150\n",
      "34/34 [==============================] - 0s 476us/step - loss: 8.3060e-04\n",
      "Epoch 49/150\n",
      "34/34 [==============================] - 0s 477us/step - loss: 8.2768e-04\n",
      "Epoch 50/150\n",
      "34/34 [==============================] - 0s 457us/step - loss: 8.2277e-04\n",
      "Epoch 51/150\n",
      "34/34 [==============================] - 0s 456us/step - loss: 8.1893e-04\n",
      "Epoch 52/150\n",
      "34/34 [==============================] - 0s 488us/step - loss: 8.1476e-04\n",
      "Epoch 53/150\n",
      "34/34 [==============================] - 0s 473us/step - loss: 8.0988e-04\n",
      "Epoch 54/150\n",
      "34/34 [==============================] - 0s 450us/step - loss: 8.0546e-04\n",
      "Epoch 55/150\n",
      "34/34 [==============================] - 0s 500us/step - loss: 8.0080e-04\n",
      "Epoch 56/150\n",
      "34/34 [==============================] - 0s 657us/step - loss: 7.9625e-04\n",
      "Epoch 57/150\n",
      "34/34 [==============================] - 0s 469us/step - loss: 7.9230e-04\n",
      "Epoch 58/150\n",
      "34/34 [==============================] - 0s 461us/step - loss: 7.8850e-04\n",
      "Epoch 59/150\n",
      "34/34 [==============================] - 0s 468us/step - loss: 7.8519e-04\n",
      "Epoch 60/150\n",
      "34/34 [==============================] - 0s 431us/step - loss: 7.8224e-04\n",
      "Epoch 61/150\n",
      "34/34 [==============================] - 0s 428us/step - loss: 7.7946e-04\n",
      "Epoch 62/150\n",
      "34/34 [==============================] - 0s 404us/step - loss: 7.7695e-04\n",
      "Epoch 63/150\n",
      "34/34 [==============================] - 0s 512us/step - loss: 7.7457e-04\n",
      "Epoch 64/150\n",
      "34/34 [==============================] - 0s 544us/step - loss: 7.7236e-04\n",
      "Epoch 65/150\n",
      "34/34 [==============================] - 0s 566us/step - loss: 7.7031e-04\n",
      "Epoch 66/150\n",
      "34/34 [==============================] - 0s 647us/step - loss: 7.6837e-04\n",
      "Epoch 67/150\n",
      "34/34 [==============================] - 0s 463us/step - loss: 7.6657e-04\n",
      "Epoch 68/150\n",
      "34/34 [==============================] - 0s 487us/step - loss: 7.6486e-04\n",
      "Epoch 69/150\n",
      "34/34 [==============================] - 0s 492us/step - loss: 7.6324e-04\n",
      "Epoch 70/150\n",
      "34/34 [==============================] - 0s 435us/step - loss: 7.6171e-04\n",
      "Epoch 71/150\n",
      "34/34 [==============================] - 0s 389us/step - loss: 7.6021e-04\n",
      "Epoch 72/150\n",
      "34/34 [==============================] - 0s 346us/step - loss: 7.5878e-04\n",
      "Epoch 73/150\n",
      "34/34 [==============================] - 0s 418us/step - loss: 7.5739e-04\n",
      "Epoch 74/150\n",
      "34/34 [==============================] - 0s 430us/step - loss: 7.5604e-04\n",
      "Epoch 75/150\n",
      "34/34 [==============================] - 0s 400us/step - loss: 7.5472e-04\n",
      "Epoch 76/150\n",
      "34/34 [==============================] - 0s 364us/step - loss: 7.5343e-04\n",
      "Epoch 77/150\n",
      "34/34 [==============================] - 0s 375us/step - loss: 7.5217e-04\n",
      "Epoch 78/150\n",
      "34/34 [==============================] - 0s 358us/step - loss: 7.5093e-04\n",
      "Epoch 79/150\n",
      "34/34 [==============================] - 0s 415us/step - loss: 7.4970e-04\n",
      "Epoch 80/150\n",
      "34/34 [==============================] - 0s 399us/step - loss: 7.4850e-04\n",
      "Epoch 81/150\n",
      "34/34 [==============================] - 0s 363us/step - loss: 7.4731e-04\n",
      "Epoch 82/150\n",
      "34/34 [==============================] - 0s 408us/step - loss: 7.4614e-04\n",
      "Epoch 83/150\n",
      "34/34 [==============================] - 0s 463us/step - loss: 7.4498e-04\n",
      "Epoch 84/150\n",
      "34/34 [==============================] - 0s 500us/step - loss: 7.4384e-04\n",
      "Epoch 85/150\n",
      "34/34 [==============================] - 0s 383us/step - loss: 7.4269e-04\n",
      "Epoch 86/150\n",
      "34/34 [==============================] - 0s 421us/step - loss: 7.4158e-04\n",
      "Epoch 87/150\n",
      "34/34 [==============================] - 0s 378us/step - loss: 7.4044e-04\n",
      "Epoch 88/150\n",
      "34/34 [==============================] - 0s 420us/step - loss: 7.3935e-04\n",
      "Epoch 89/150\n",
      "34/34 [==============================] - 0s 414us/step - loss: 7.3826e-04\n",
      "Epoch 90/150\n",
      "34/34 [==============================] - 0s 368us/step - loss: 7.3715e-04\n",
      "Epoch 91/150\n",
      "34/34 [==============================] - 0s 399us/step - loss: 7.3608e-04\n",
      "Epoch 92/150\n",
      "34/34 [==============================] - 0s 452us/step - loss: 7.3500e-04\n",
      "Epoch 93/150\n",
      "34/34 [==============================] - 0s 425us/step - loss: 7.3393e-04\n",
      "Epoch 94/150\n",
      "34/34 [==============================] - 0s 441us/step - loss: 7.3286e-04\n",
      "Epoch 95/150\n",
      "34/34 [==============================] - 0s 412us/step - loss: 7.3180e-04\n",
      "Epoch 96/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 429us/step - loss: 7.3075e-04\n",
      "Epoch 97/150\n",
      "34/34 [==============================] - 0s 408us/step - loss: 7.2970e-04\n",
      "Epoch 98/150\n",
      "34/34 [==============================] - 0s 430us/step - loss: 7.2866e-04\n",
      "Epoch 99/150\n",
      "34/34 [==============================] - 0s 372us/step - loss: 7.2762e-04\n",
      "Epoch 100/150\n",
      "34/34 [==============================] - 0s 420us/step - loss: 7.2658e-04\n",
      "Epoch 101/150\n",
      "34/34 [==============================] - 0s 471us/step - loss: 7.2554e-04\n",
      "Epoch 102/150\n",
      "34/34 [==============================] - 0s 418us/step - loss: 7.2451e-04\n",
      "Epoch 103/150\n",
      "34/34 [==============================] - 0s 401us/step - loss: 7.2347e-04\n",
      "Epoch 104/150\n",
      "34/34 [==============================] - 0s 385us/step - loss: 7.2246e-04\n",
      "Epoch 105/150\n",
      "34/34 [==============================] - 0s 353us/step - loss: 7.2142e-04\n",
      "Epoch 106/150\n",
      "34/34 [==============================] - 0s 380us/step - loss: 7.2040e-04\n",
      "Epoch 107/150\n",
      "34/34 [==============================] - 0s 380us/step - loss: 7.1938e-04\n",
      "Epoch 108/150\n",
      "34/34 [==============================] - 0s 360us/step - loss: 7.1834e-04\n",
      "Epoch 109/150\n",
      "34/34 [==============================] - 0s 378us/step - loss: 7.1733e-04\n",
      "Epoch 110/150\n",
      "34/34 [==============================] - 0s 340us/step - loss: 7.1629e-04\n",
      "Epoch 111/150\n",
      "34/34 [==============================] - 0s 401us/step - loss: 7.1526e-04\n",
      "Epoch 112/150\n",
      "34/34 [==============================] - 0s 368us/step - loss: 7.1423e-04\n",
      "Epoch 113/150\n",
      "34/34 [==============================] - 0s 363us/step - loss: 7.1319e-04\n",
      "Epoch 114/150\n",
      "34/34 [==============================] - 0s 375us/step - loss: 7.1214e-04\n",
      "Epoch 115/150\n",
      "34/34 [==============================] - 0s 337us/step - loss: 7.1110e-04\n",
      "Epoch 116/150\n",
      "34/34 [==============================] - 0s 371us/step - loss: 7.1004e-04\n",
      "Epoch 117/150\n",
      "34/34 [==============================] - 0s 355us/step - loss: 7.0898e-04\n",
      "Epoch 118/150\n",
      "34/34 [==============================] - 0s 386us/step - loss: 7.0790e-04\n",
      "Epoch 119/150\n",
      "34/34 [==============================] - 0s 386us/step - loss: 7.0682e-04\n",
      "Epoch 120/150\n",
      "34/34 [==============================] - 0s 383us/step - loss: 7.0573e-04\n",
      "Epoch 121/150\n",
      "34/34 [==============================] - 0s 436us/step - loss: 7.0462e-04\n",
      "Epoch 122/150\n",
      "34/34 [==============================] - 0s 363us/step - loss: 7.0349e-04\n",
      "Epoch 123/150\n",
      "34/34 [==============================] - 0s 312us/step - loss: 7.0234e-04\n",
      "Epoch 124/150\n",
      "34/34 [==============================] - 0s 334us/step - loss: 7.0118e-04\n",
      "Epoch 125/150\n",
      "34/34 [==============================] - 0s 326us/step - loss: 6.9997e-04\n",
      "Epoch 126/150\n",
      "34/34 [==============================] - 0s 386us/step - loss: 6.9875e-04\n",
      "Epoch 127/150\n",
      "34/34 [==============================] - 0s 376us/step - loss: 6.9747e-04\n",
      "Epoch 128/150\n",
      "34/34 [==============================] - 0s 356us/step - loss: 6.9614e-04\n",
      "Epoch 129/150\n",
      "34/34 [==============================] - 0s 323us/step - loss: 6.9473e-04\n",
      "Epoch 130/150\n",
      "34/34 [==============================] - 0s 404us/step - loss: 6.9324e-04\n",
      "Epoch 131/150\n",
      "34/34 [==============================] - 0s 427us/step - loss: 6.9169e-04\n",
      "Epoch 132/150\n",
      "34/34 [==============================] - 0s 397us/step - loss: 6.9135e-04\n",
      "Epoch 133/150\n",
      "34/34 [==============================] - 0s 385us/step - loss: 6.3190e-04\n",
      "Epoch 134/150\n",
      "34/34 [==============================] - 0s 382us/step - loss: 6.9742e-04\n",
      "Epoch 135/150\n",
      "34/34 [==============================] - 0s 335us/step - loss: 6.7465e-04\n",
      "Epoch 136/150\n",
      "34/34 [==============================] - 0s 378us/step - loss: 6.2111e-04\n",
      "Epoch 137/150\n",
      "34/34 [==============================] - 0s 392us/step - loss: 6.1871e-04\n",
      "Epoch 138/150\n",
      "34/34 [==============================] - 0s 395us/step - loss: 7.7761e-04\n",
      "Epoch 139/150\n",
      "34/34 [==============================] - 0s 379us/step - loss: 8.3869e-04\n",
      "Epoch 140/150\n",
      "34/34 [==============================] - 0s 446us/step - loss: 8.7931e-04\n",
      "Epoch 141/150\n",
      "34/34 [==============================] - 0s 424us/step - loss: 0.0010\n",
      "Epoch 142/150\n",
      "34/34 [==============================] - 0s 365us/step - loss: 7.1348e-04\n",
      "Epoch 143/150\n",
      "34/34 [==============================] - 0s 364us/step - loss: 6.9646e-04\n",
      "Epoch 144/150\n",
      "34/34 [==============================] - 0s 399us/step - loss: 9.4631e-04\n",
      "Epoch 145/150\n",
      "34/34 [==============================] - 0s 390us/step - loss: 8.7883e-04\n",
      "Epoch 146/150\n",
      "34/34 [==============================] - 0s 378us/step - loss: 8.1999e-04\n",
      "Epoch 147/150\n",
      "34/34 [==============================] - 0s 356us/step - loss: 0.0011\n",
      "Epoch 148/150\n",
      "34/34 [==============================] - 0s 368us/step - loss: 9.3506e-04\n",
      "Epoch 149/150\n",
      "34/34 [==============================] - 0s 394us/step - loss: 6.6320e-04\n",
      "Epoch 150/150\n",
      "34/34 [==============================] - 0s 406us/step - loss: 7.2330e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4c5bab10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def deep_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=17, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    model.layers[0].set_weights([np.zeros([17,8])+0.01, np.zeros([8,])])\n",
    "    model.layers[1].set_weights([np.zeros([8,8])+0.01, np.zeros([8,])])\n",
    "    model.layers[2].set_weights([np.zeros([8,1])+0.01, np.array([0.00])])\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "deepNetwork = deep_model()\n",
    "\n",
    "deepNetwork.fit(X_train, y_train, epochs=150, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the testing set: 0.001011198852211237\n"
     ]
    }
   ],
   "source": [
    "test_loss = deepNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "print('The loss on the testing set:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the RELU loss function seems to improve the loss ever so slightly more... Perhaps this is due to the fact that the first layer can pick out the significant combinations (normalized) and the second can evaluate the relative significance between them. Regardless, the improvement is not that significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test set number 0\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Starting test set number 1\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Starting test set number 2\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Starting test set number 3\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Starting test set number 4\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Starting test set number 5\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Starting test set number 6\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Starting test set number 7\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Starting test set number 8\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Starting test set number 9\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n"
     ]
    }
   ],
   "source": [
    "# This will take a long time to run\n",
    "reg_mse = []\n",
    "base_mse = []\n",
    "thin_mse = []\n",
    "deep_mse = []\n",
    "trivial_mse = []\n",
    "for i in range(10):\n",
    "    print('Starting test set number', i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33)\n",
    "    reg = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n",
    "\n",
    "    pred_values = reg.predict(X_train)\n",
    "    \n",
    "    print('Training base network...')\n",
    "    baseNetwork = base_model()\n",
    "    baseNetwork.fit(X_train, y_train, epochs=100, batch_size=5, shuffle=False, verbose=0)\n",
    "    base_loss = baseNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print('Training thin network...')\n",
    "    thinNetwork = thin_model()\n",
    "    thinNetwork.fit(X_train, y_train, epochs=100, batch_size=5, shuffle=False, verbose=0)\n",
    "    thin_loss = thinNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print('Training deep network...')\n",
    "    deepNetwork = deep_model()\n",
    "    deepNetwork.fit(X_train, y_train, epochs=100, batch_size=5, shuffle=False, verbose=0)\n",
    "    deep_loss = deepNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    \n",
    "    reg_mse = np.append(reg_mse, np.sqrt( np.mean((pred_values - y_train) ** 2 ) ))\n",
    "    base_mse = np.append(base_mse, base_loss)\n",
    "    thin_mse = np.append(thin_mse, thin_loss)\n",
    "    deep_mse = np.append(deep_mse, deep_loss)\n",
    "    trivial_mse = np.append(trivial_mse, np.sqrt( np.mean( (np.mean(y_train) - y_test)**2 ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinReg</th>\n",
       "      <th>Trivial</th>\n",
       "      <th>Base</th>\n",
       "      <th>Thin</th>\n",
       "      <th>Deep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.038345</td>\n",
       "      <td>0.030671</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.001230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.043885</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.029122</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.000764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.036728</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.000909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.017549</td>\n",
       "      <td>0.038345</td>\n",
       "      <td>0.006746</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.001096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.019332</td>\n",
       "      <td>0.041675</td>\n",
       "      <td>0.040481</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.001556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.021058</td>\n",
       "      <td>0.044101</td>\n",
       "      <td>0.139833</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.001942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LinReg    Trivial       Base       Thin       Deep\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000\n",
       "mean    0.017971   0.038345   0.030671   0.001445   0.001230\n",
       "std     0.002156   0.004628   0.043885   0.000475   0.000420\n",
       "min     0.014355   0.029122   0.002078   0.000667   0.000764\n",
       "25%     0.016765   0.036728   0.004562   0.001109   0.000909\n",
       "50%     0.017549   0.038345   0.006746   0.001581   0.001096\n",
       "75%     0.019332   0.041675   0.040481   0.001747   0.001556\n",
       "max     0.021058   0.044101   0.139833   0.002067   0.001942"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = np.stack([reg_mse, trivial_mse, base_mse, thin_mse, deep_mse])\n",
    "\n",
    "errdf = pd.DataFrame(error.T, columns=['LinReg', 'Trivial', 'Base', 'Thin', 'Deep'])\n",
    "errdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = errdf.plot.hist(bins=100, alpha=0.5, title='Distribution of MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
