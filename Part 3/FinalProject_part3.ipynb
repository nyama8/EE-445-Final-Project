{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 445 Final Project (Part 3): Modeling effectiveness of government response\n",
    "Nicholas Yama, Spring 2020\n",
    "\n",
    "## Project goal\n",
    "\n",
    "At the time of writing (07 May 2020) the United States is beginning to see the number of cases slowing, resulting in increase in desire by a vocal minority to return to normal operations. This would involve the uplifing of wide-scale quarantine and stay-at-home orders, of which the evidence indicates has done significant good in slowing the spread of infections. Many arguments are being levied against the effectiveness of these restrictions which are often of dubious scientific merit.\n",
    "\n",
    "The effectiveness of the government response has varied between states in curbing the number of cases and, due to the lack of national policy from the federal government, will likey be exacerbated as individual states begin to relax these measures. The goal of this project is to model the effectiveness of the government's response at a state level in terms of initial growth rates of the infection.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "The number of infected individuals $N$ in its early stages generally can be modeled by an exponential curve\n",
    "\n",
    "$$N=\\exp(r(t-t_0))$$\n",
    "\n",
    "where $r$ is the growth rate, $t$ is the time, and $t_0$ is the time until the first infection. The growth rate $r$ can then be determined by a linear regression on $\\ln(N) = r(t - t_0)$.\n",
    "\n",
    "This growth rate $r$ will be predicted using a linear regression-type network for the first 6 weeks starting March 1st (a day later the US would reach the first confirmed 100 cases).\n",
    "\n",
    "The parameters of interest include:\n",
    "1. The testing rate integrated weekly (so the number of tests administered from March 1 to a given week) normalized by the population (# tests adminstered/population, for 6 weeks).\n",
    "2. Time from March 1 to declare a \"state of emergency\"\n",
    "3. Time from March 1 to issue a stay-at-home order (state level)\n",
    "4. Maximum number of persons at a gathering (in cases where a distinction is made between private and public gatherings, the maximum is taken)\n",
    "5. Travel restrictions (1 = none, 2 = partial, 3 = mandatory).\n",
    "6. School closures (1 = none, 2 = partial, 3 = all).\n",
    "7. Daycare closures (1 = none, 2 = partial, 3 = all).\n",
    "8. Restauraunts closures (1 = none, 2 = partial, 3 = all).\n",
    "8. Non-essential retail closures (1 = none, 2 = partial, 3 = all).\n",
    "9. Governor political affiliation (1 = democrat, 2 = republican).\n",
    "10. Population density (per square mile, control parameter).\n",
    "\n",
    "When parameters associated with time of response were not achieved within the time frame provided (eg. no stay at home order issued within the 6 weeks) a value of 50 is assigned (greater than maximal value of 42).\n",
    "\n",
    "Each data point will represent an individual state. Only the 50 states and the District of Columbia will be considered, territories of the US such as American Samoa and Puerto Rico will not be used.\n",
    "\n",
    "### Data sources\n",
    "[1] Target data to calculate the growth rate of cases, and the testing data was obtained via the COVID Tracking project: https://covidtracking.com/. The data was collected on May 06 (but data was only used up to April 12 so the collection date does not matter).\n",
    "\n",
    "[2] Parameters 2-9 were obtained from the Wikipedia page: https://en.wikipedia.org/wiki/U.S._state_and_local_government_response_to_the_COVID-19_pandemic, I processed the data in Microsoft Excel, using a search and replace on terms corresponding to the values above.\n",
    "\n",
    "[3] Governor partisanship (parameter 9) was found: https://en.wikipedia.org/wiki/List_of_United_States_governors\n",
    "\n",
    "[4] State population statistics was found from the US 2010 Census projections for 2019. The full dataset can be found: https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-total.html\n",
    "\n",
    "# Loading data\n",
    "The data is being loaded from my Github repository. The data was initially processed from the sources above using the notebook located: https://github.com/nyama8/EE-445-Final-Project/blob/master/Part%202/FinalProject_part2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tests 1</th>\n",
       "      <th>Tests 2</th>\n",
       "      <th>Tests 3</th>\n",
       "      <th>Tests 4</th>\n",
       "      <th>Tests 5</th>\n",
       "      <th>Tests 6</th>\n",
       "      <th>Tests 7</th>\n",
       "      <th>density</th>\n",
       "      <th>emergency</th>\n",
       "      <th>quarantine</th>\n",
       "      <th>gatherings</th>\n",
       "      <th>travel</th>\n",
       "      <th>school</th>\n",
       "      <th>daycares</th>\n",
       "      <th>restauraunts</th>\n",
       "      <th>retail</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>0.145489</td>\n",
       "      <td>1.380985</td>\n",
       "      <td>8.824113</td>\n",
       "      <td>30.812967</td>\n",
       "      <td>63.724578</td>\n",
       "      <td>102.464746</td>\n",
       "      <td>412.058824</td>\n",
       "      <td>8.921569</td>\n",
       "      <td>29.941176</td>\n",
       "      <td>11.588235</td>\n",
       "      <td>1.862745</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.764706</td>\n",
       "      <td>2.941176</td>\n",
       "      <td>2.725490</td>\n",
       "      <td>1.490196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.020440</td>\n",
       "      <td>0.454149</td>\n",
       "      <td>3.618019</td>\n",
       "      <td>11.500424</td>\n",
       "      <td>27.779183</td>\n",
       "      <td>54.629108</td>\n",
       "      <td>90.068199</td>\n",
       "      <td>1536.591050</td>\n",
       "      <td>3.632317</td>\n",
       "      <td>9.213928</td>\n",
       "      <td>13.661883</td>\n",
       "      <td>0.916943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.586114</td>\n",
       "      <td>0.310597</td>\n",
       "      <td>0.634931</td>\n",
       "      <td>0.543049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014126</td>\n",
       "      <td>0.420049</td>\n",
       "      <td>2.752207</td>\n",
       "      <td>6.704623</td>\n",
       "      <td>11.529469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006828</td>\n",
       "      <td>0.246191</td>\n",
       "      <td>2.712900</td>\n",
       "      <td>15.046188</td>\n",
       "      <td>37.274537</td>\n",
       "      <td>58.549733</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047543</td>\n",
       "      <td>0.412394</td>\n",
       "      <td>6.138395</td>\n",
       "      <td>25.719804</td>\n",
       "      <td>49.315151</td>\n",
       "      <td>79.901797</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121059</td>\n",
       "      <td>1.170444</td>\n",
       "      <td>9.018394</td>\n",
       "      <td>38.097159</td>\n",
       "      <td>69.805266</td>\n",
       "      <td>114.818142</td>\n",
       "      <td>226.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.145767</td>\n",
       "      <td>3.226572</td>\n",
       "      <td>25.359516</td>\n",
       "      <td>60.308136</td>\n",
       "      <td>170.019023</td>\n",
       "      <td>379.349636</td>\n",
       "      <td>640.796553</td>\n",
       "      <td>11011.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tests 1    Tests 2    Tests 3    Tests 4     Tests 5     Tests 6  \\\n",
       "count  51.000000  51.000000  51.000000  51.000000   51.000000   51.000000   \n",
       "mean    0.003220   0.145489   1.380985   8.824113   30.812967   63.724578   \n",
       "std     0.020440   0.454149   3.618019  11.500424   27.779183   54.629108   \n",
       "min     0.000000   0.000000   0.014126   0.420049    2.752207    6.704623   \n",
       "25%     0.000000   0.006828   0.246191   2.712900   15.046188   37.274537   \n",
       "50%     0.000000   0.047543   0.412394   6.138395   25.719804   49.315151   \n",
       "75%     0.000000   0.121059   1.170444   9.018394   38.097159   69.805266   \n",
       "max     0.145767   3.226572  25.359516  60.308136  170.019023  379.349636   \n",
       "\n",
       "          Tests 7       density  emergency  quarantine  gatherings     travel  \\\n",
       "count   51.000000     51.000000  51.000000   51.000000   51.000000  51.000000   \n",
       "mean   102.464746    412.058824   8.921569   29.941176   11.588235   1.862745   \n",
       "std     90.068199   1536.591050   3.632317    9.213928   13.661883   0.916943   \n",
       "min     11.529469      1.000000  -1.000000   18.000000    1.000000   1.000000   \n",
       "25%     58.549733     47.500000   7.500000   23.000000    1.000000   1.000000   \n",
       "50%     79.901797    106.000000  10.000000   27.000000   10.000000   2.000000   \n",
       "75%    114.818142    226.500000  12.000000   32.500000   10.000000   3.000000   \n",
       "max    640.796553  11011.000000  15.000000   50.000000   50.000000   3.000000   \n",
       "\n",
       "       school   daycares  restauraunts     retail      party  \n",
       "count    51.0  51.000000     51.000000  51.000000  51.000000  \n",
       "mean      3.0   2.764706      2.941176   2.725490   1.490196  \n",
       "std       0.0   0.586114      0.310597   0.634931   0.543049  \n",
       "min       3.0   1.000000      1.000000   1.000000   0.000000  \n",
       "25%       3.0   3.000000      3.000000   3.000000   1.000000  \n",
       "50%       3.0   3.000000      3.000000   3.000000   2.000000  \n",
       "75%       3.0   3.000000      3.000000   3.000000   2.000000  \n",
       "max       3.0   3.000000      3.000000   3.000000   2.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load parameters dataset\n",
    "url = 'https://raw.githubusercontent.com/nyama8/EE-445-Final-Project/master/Processed%20data/sampledata.csv'\n",
    "datadf = pd.read_csv(url, error_bad_lines=False)\n",
    "datadf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.216806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.035466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.134223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.190891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.219613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.239307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.299372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rate\n",
       "count  51.000000\n",
       "mean    0.216806\n",
       "std     0.035466\n",
       "min     0.134223\n",
       "25%     0.190891\n",
       "50%     0.219613\n",
       "75%     0.239307\n",
       "max     0.299372"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load target dataset\n",
    "url = 'https://raw.githubusercontent.com/nyama8/EE-445-Final-Project/master/Processed%20data/sampletarget.csv'\n",
    "targetdf = pd.read_csv(url, error_bad_lines=False)\n",
    "targetdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datadf.to_numpy()\n",
    "target = targetdf.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with a feed-forward network\n",
    "\n",
    "As an intial pass at this problem, I used linear regression to model the relation between the parameters and infection rate $r$. This would represent a network with 13 input nodes and a single output node with a mean squared error loss function. Here I use a more advanced feed-forward network approach to address the problem.\n",
    "\n",
    "In the following approaches, I used the tutorial available here: https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/, to help with syntax and how to actually implement the network.\n",
    "\n",
    "\n",
    "## Recreating explicit linear regression with a network\n",
    "Previously I used the LinearRegression() function which determines the optimal estimator directly. Here I implement a network with the same topology that approximates the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network model\n",
    "def base_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=17, kernel_initializer='normal'))\n",
    "    \n",
    "    model.layers[0].set_weights([np.zeros([17,1])+0.01, np.array([0.00])])\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/150\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 402.6320\n",
      "Epoch 2/150\n",
      "34/34 [==============================] - 0s 240us/step - loss: 161.3381\n",
      "Epoch 3/150\n",
      "34/34 [==============================] - 0s 294us/step - loss: 57.0778\n",
      "Epoch 4/150\n",
      "34/34 [==============================] - 0s 323us/step - loss: 13.3710\n",
      "Epoch 5/150\n",
      "34/34 [==============================] - 0s 328us/step - loss: 2.5354\n",
      "Epoch 6/150\n",
      "34/34 [==============================] - 0s 292us/step - loss: 2.6779\n",
      "Epoch 7/150\n",
      "34/34 [==============================] - 0s 333us/step - loss: 2.8390\n",
      "Epoch 8/150\n",
      "34/34 [==============================] - 0s 302us/step - loss: 1.5361\n",
      "Epoch 9/150\n",
      "34/34 [==============================] - 0s 330us/step - loss: 0.4017\n",
      "Epoch 10/150\n",
      "34/34 [==============================] - 0s 311us/step - loss: 0.0816\n",
      "Epoch 11/150\n",
      "34/34 [==============================] - 0s 277us/step - loss: 0.1194\n",
      "Epoch 12/150\n",
      "34/34 [==============================] - 0s 290us/step - loss: 0.1093\n",
      "Epoch 13/150\n",
      "34/34 [==============================] - 0s 303us/step - loss: 0.0446\n",
      "Epoch 14/150\n",
      "34/34 [==============================] - 0s 302us/step - loss: 0.0132\n",
      "Epoch 15/150\n",
      "34/34 [==============================] - 0s 382us/step - loss: 0.0141\n",
      "Epoch 16/150\n",
      "34/34 [==============================] - 0s 344us/step - loss: 0.0155\n",
      "Epoch 17/150\n",
      "34/34 [==============================] - 0s 273us/step - loss: 0.0117\n",
      "Epoch 18/150\n",
      "34/34 [==============================] - 0s 303us/step - loss: 0.0093\n",
      "Epoch 19/150\n",
      "34/34 [==============================] - 0s 323us/step - loss: 0.0090\n",
      "Epoch 20/150\n",
      "34/34 [==============================] - 0s 296us/step - loss: 0.0086\n",
      "Epoch 21/150\n",
      "34/34 [==============================] - 0s 308us/step - loss: 0.0079\n",
      "Epoch 22/150\n",
      "34/34 [==============================] - 0s 339us/step - loss: 0.0073\n",
      "Epoch 23/150\n",
      "34/34 [==============================] - 0s 370us/step - loss: 0.0070\n",
      "Epoch 24/150\n",
      "34/34 [==============================] - 0s 255us/step - loss: 0.0066\n",
      "Epoch 25/150\n",
      "34/34 [==============================] - 0s 333us/step - loss: 0.0063\n",
      "Epoch 26/150\n",
      "34/34 [==============================] - 0s 284us/step - loss: 0.0060\n",
      "Epoch 27/150\n",
      "34/34 [==============================] - 0s 277us/step - loss: 0.0058\n",
      "Epoch 28/150\n",
      "34/34 [==============================] - 0s 341us/step - loss: 0.0056\n",
      "Epoch 29/150\n",
      "34/34 [==============================] - 0s 356us/step - loss: 0.0053\n",
      "Epoch 30/150\n",
      "34/34 [==============================] - 0s 447us/step - loss: 0.0051\n",
      "Epoch 31/150\n",
      "34/34 [==============================] - 0s 258us/step - loss: 0.0050\n",
      "Epoch 32/150\n",
      "34/34 [==============================] - 0s 343us/step - loss: 0.0048\n",
      "Epoch 33/150\n",
      "34/34 [==============================] - 0s 316us/step - loss: 0.0046\n",
      "Epoch 34/150\n",
      "34/34 [==============================] - 0s 263us/step - loss: 0.0045\n",
      "Epoch 35/150\n",
      "34/34 [==============================] - 0s 353us/step - loss: 0.0043\n",
      "Epoch 36/150\n",
      "34/34 [==============================] - 0s 355us/step - loss: 0.0042\n",
      "Epoch 37/150\n",
      "34/34 [==============================] - 0s 274us/step - loss: 0.0041\n",
      "Epoch 38/150\n",
      "34/34 [==============================] - 0s 297us/step - loss: 0.0040\n",
      "Epoch 39/150\n",
      "34/34 [==============================] - 0s 348us/step - loss: 0.0038\n",
      "Epoch 40/150\n",
      "34/34 [==============================] - 0s 318us/step - loss: 0.0037\n",
      "Epoch 41/150\n",
      "34/34 [==============================] - 0s 292us/step - loss: 0.0036\n",
      "Epoch 42/150\n",
      "34/34 [==============================] - 0s 324us/step - loss: 0.0035\n",
      "Epoch 43/150\n",
      "34/34 [==============================] - 0s 336us/step - loss: 0.0035\n",
      "Epoch 44/150\n",
      "34/34 [==============================] - 0s 313us/step - loss: 0.0034\n",
      "Epoch 45/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 0.0033\n",
      "Epoch 46/150\n",
      "34/34 [==============================] - 0s 299us/step - loss: 0.0032\n",
      "Epoch 47/150\n",
      "34/34 [==============================] - 0s 269us/step - loss: 0.0031\n",
      "Epoch 48/150\n",
      "34/34 [==============================] - 0s 321us/step - loss: 0.0031\n",
      "Epoch 49/150\n",
      "34/34 [==============================] - 0s 388us/step - loss: 0.0030\n",
      "Epoch 50/150\n",
      "34/34 [==============================] - 0s 349us/step - loss: 0.0030\n",
      "Epoch 51/150\n",
      "34/34 [==============================] - 0s 283us/step - loss: 0.0029\n",
      "Epoch 52/150\n",
      "34/34 [==============================] - 0s 364us/step - loss: 0.0028\n",
      "Epoch 53/150\n",
      "34/34 [==============================] - 0s 324us/step - loss: 0.0028\n",
      "Epoch 54/150\n",
      "34/34 [==============================] - 0s 261us/step - loss: 0.0028\n",
      "Epoch 55/150\n",
      "34/34 [==============================] - 0s 272us/step - loss: 0.0027\n",
      "Epoch 56/150\n",
      "34/34 [==============================] - 0s 255us/step - loss: 0.0027\n",
      "Epoch 57/150\n",
      "34/34 [==============================] - 0s 294us/step - loss: 0.0026\n",
      "Epoch 58/150\n",
      "34/34 [==============================] - 0s 351us/step - loss: 0.0026\n",
      "Epoch 59/150\n",
      "34/34 [==============================] - 0s 308us/step - loss: 0.0026\n",
      "Epoch 60/150\n",
      "34/34 [==============================] - 0s 283us/step - loss: 0.0025\n",
      "Epoch 61/150\n",
      "34/34 [==============================] - 0s 293us/step - loss: 0.0025\n",
      "Epoch 62/150\n",
      "34/34 [==============================] - 0s 248us/step - loss: 0.0025\n",
      "Epoch 63/150\n",
      "34/34 [==============================] - 0s 269us/step - loss: 0.0024\n",
      "Epoch 64/150\n",
      "34/34 [==============================] - 0s 257us/step - loss: 0.0024\n",
      "Epoch 65/150\n",
      "34/34 [==============================] - 0s 277us/step - loss: 0.0024\n",
      "Epoch 66/150\n",
      "34/34 [==============================] - 0s 274us/step - loss: 0.0023\n",
      "Epoch 67/150\n",
      "34/34 [==============================] - 0s 241us/step - loss: 0.0023\n",
      "Epoch 68/150\n",
      "34/34 [==============================] - 0s 295us/step - loss: 0.0023\n",
      "Epoch 69/150\n",
      "34/34 [==============================] - 0s 260us/step - loss: 0.0023\n",
      "Epoch 70/150\n",
      "34/34 [==============================] - 0s 280us/step - loss: 0.0023\n",
      "Epoch 71/150\n",
      "34/34 [==============================] - 0s 251us/step - loss: 0.0022\n",
      "Epoch 72/150\n",
      "34/34 [==============================] - 0s 237us/step - loss: 0.0022\n",
      "Epoch 73/150\n",
      "34/34 [==============================] - 0s 272us/step - loss: 0.0022\n",
      "Epoch 74/150\n",
      "34/34 [==============================] - 0s 251us/step - loss: 0.0022\n",
      "Epoch 75/150\n",
      "34/34 [==============================] - 0s 357us/step - loss: 0.0022\n",
      "Epoch 76/150\n",
      "34/34 [==============================] - 0s 309us/step - loss: 0.0021\n",
      "Epoch 77/150\n",
      "34/34 [==============================] - 0s 234us/step - loss: 0.0021\n",
      "Epoch 78/150\n",
      "34/34 [==============================] - 0s 312us/step - loss: 0.0021\n",
      "Epoch 79/150\n",
      "34/34 [==============================] - 0s 266us/step - loss: 0.0021\n",
      "Epoch 80/150\n",
      "34/34 [==============================] - 0s 271us/step - loss: 0.0021\n",
      "Epoch 81/150\n",
      "34/34 [==============================] - 0s 261us/step - loss: 0.0021\n",
      "Epoch 82/150\n",
      "34/34 [==============================] - 0s 255us/step - loss: 0.0020\n",
      "Epoch 83/150\n",
      "34/34 [==============================] - 0s 308us/step - loss: 0.0020\n",
      "Epoch 84/150\n",
      "34/34 [==============================] - 0s 299us/step - loss: 0.0020\n",
      "Epoch 85/150\n",
      "34/34 [==============================] - 0s 260us/step - loss: 0.0020\n",
      "Epoch 86/150\n",
      "34/34 [==============================] - 0s 255us/step - loss: 0.0020\n",
      "Epoch 87/150\n",
      "34/34 [==============================] - 0s 237us/step - loss: 0.0020\n",
      "Epoch 88/150\n",
      "34/34 [==============================] - 0s 310us/step - loss: 0.0020\n",
      "Epoch 89/150\n",
      "34/34 [==============================] - 0s 262us/step - loss: 0.0019\n",
      "Epoch 90/150\n",
      "34/34 [==============================] - 0s 257us/step - loss: 0.0019\n",
      "Epoch 91/150\n",
      "34/34 [==============================] - 0s 240us/step - loss: 0.0019\n",
      "Epoch 92/150\n",
      "34/34 [==============================] - 0s 234us/step - loss: 0.0019\n",
      "Epoch 93/150\n",
      "34/34 [==============================] - 0s 337us/step - loss: 0.0019\n",
      "Epoch 94/150\n",
      "34/34 [==============================] - 0s 247us/step - loss: 0.0019\n",
      "Epoch 95/150\n",
      "34/34 [==============================] - 0s 384us/step - loss: 0.0019\n",
      "Epoch 96/150\n",
      "34/34 [==============================] - 0s 325us/step - loss: 0.0019\n",
      "Epoch 97/150\n",
      "34/34 [==============================] - 0s 245us/step - loss: 0.0018\n",
      "Epoch 98/150\n",
      "34/34 [==============================] - 0s 248us/step - loss: 0.0018\n",
      "Epoch 99/150\n",
      "34/34 [==============================] - 0s 283us/step - loss: 0.0018\n",
      "Epoch 100/150\n",
      "34/34 [==============================] - 0s 231us/step - loss: 0.0018\n",
      "Epoch 101/150\n",
      "34/34 [==============================] - 0s 316us/step - loss: 0.0018\n",
      "Epoch 102/150\n",
      "34/34 [==============================] - 0s 248us/step - loss: 0.0018\n",
      "Epoch 103/150\n",
      "34/34 [==============================] - 0s 274us/step - loss: 0.0018\n",
      "Epoch 104/150\n",
      "34/34 [==============================] - 0s 263us/step - loss: 0.0018\n",
      "Epoch 105/150\n",
      "34/34 [==============================] - 0s 253us/step - loss: 0.0017\n",
      "Epoch 106/150\n",
      "34/34 [==============================] - 0s 297us/step - loss: 0.0017\n",
      "Epoch 107/150\n",
      "34/34 [==============================] - 0s 247us/step - loss: 0.0017\n",
      "Epoch 108/150\n",
      "34/34 [==============================] - 0s 345us/step - loss: 0.0017\n",
      "Epoch 109/150\n",
      "34/34 [==============================] - 0s 284us/step - loss: 0.0017\n",
      "Epoch 110/150\n",
      "34/34 [==============================] - 0s 245us/step - loss: 0.0017\n",
      "Epoch 111/150\n",
      "34/34 [==============================] - 0s 270us/step - loss: 0.0017\n",
      "Epoch 112/150\n",
      "34/34 [==============================] - 0s 242us/step - loss: 0.0017\n",
      "Epoch 113/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 0.0017\n",
      "Epoch 114/150\n",
      "34/34 [==============================] - 0s 307us/step - loss: 0.0016\n",
      "Epoch 115/150\n",
      "34/34 [==============================] - 0s 239us/step - loss: 0.0016\n",
      "Epoch 116/150\n",
      "34/34 [==============================] - 0s 306us/step - loss: 0.0016\n",
      "Epoch 117/150\n",
      "34/34 [==============================] - 0s 249us/step - loss: 0.0016\n",
      "Epoch 118/150\n",
      "34/34 [==============================] - 0s 291us/step - loss: 0.0016\n",
      "Epoch 119/150\n",
      "34/34 [==============================] - 0s 280us/step - loss: 0.0016\n",
      "Epoch 120/150\n",
      "34/34 [==============================] - 0s 258us/step - loss: 0.0016\n",
      "Epoch 121/150\n",
      "34/34 [==============================] - 0s 317us/step - loss: 0.0016\n",
      "Epoch 122/150\n",
      "34/34 [==============================] - 0s 272us/step - loss: 0.0016\n",
      "Epoch 123/150\n",
      "34/34 [==============================] - 0s 283us/step - loss: 0.0016\n",
      "Epoch 124/150\n",
      "34/34 [==============================] - 0s 285us/step - loss: 0.0015\n",
      "Epoch 125/150\n",
      "34/34 [==============================] - 0s 231us/step - loss: 0.0015\n",
      "Epoch 126/150\n",
      "34/34 [==============================] - 0s 226us/step - loss: 0.0015\n",
      "Epoch 127/150\n",
      "34/34 [==============================] - 0s 260us/step - loss: 0.0015\n",
      "Epoch 128/150\n",
      "34/34 [==============================] - 0s 301us/step - loss: 0.0015\n",
      "Epoch 129/150\n",
      "34/34 [==============================] - 0s 320us/step - loss: 0.0015\n",
      "Epoch 130/150\n",
      "34/34 [==============================] - 0s 322us/step - loss: 0.0015\n",
      "Epoch 131/150\n",
      "34/34 [==============================] - 0s 291us/step - loss: 0.0015\n",
      "Epoch 132/150\n",
      "34/34 [==============================] - 0s 274us/step - loss: 0.0015\n",
      "Epoch 133/150\n",
      "34/34 [==============================] - 0s 297us/step - loss: 0.0015\n",
      "Epoch 134/150\n",
      "34/34 [==============================] - 0s 347us/step - loss: 0.0014\n",
      "Epoch 135/150\n",
      "34/34 [==============================] - 0s 391us/step - loss: 0.0014\n",
      "Epoch 136/150\n",
      "34/34 [==============================] - 0s 346us/step - loss: 0.0014\n",
      "Epoch 137/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 0.0014\n",
      "Epoch 138/150\n",
      "34/34 [==============================] - 0s 259us/step - loss: 0.0014\n",
      "Epoch 139/150\n",
      "34/34 [==============================] - 0s 270us/step - loss: 0.0014\n",
      "Epoch 140/150\n",
      "34/34 [==============================] - 0s 243us/step - loss: 0.0014\n",
      "Epoch 141/150\n",
      "34/34 [==============================] - 0s 287us/step - loss: 0.0014\n",
      "Epoch 142/150\n",
      "34/34 [==============================] - 0s 262us/step - loss: 0.0014\n",
      "Epoch 143/150\n",
      "34/34 [==============================] - 0s 255us/step - loss: 0.0014\n",
      "Epoch 144/150\n",
      "34/34 [==============================] - 0s 257us/step - loss: 0.0014\n",
      "Epoch 145/150\n",
      "34/34 [==============================] - 0s 226us/step - loss: 0.0013\n",
      "Epoch 146/150\n",
      "34/34 [==============================] - 0s 252us/step - loss: 0.0013\n",
      "Epoch 147/150\n",
      "34/34 [==============================] - 0s 282us/step - loss: 0.0013\n",
      "Epoch 148/150\n",
      "34/34 [==============================] - 0s 331us/step - loss: 0.0013\n",
      "Epoch 149/150\n",
      "34/34 [==============================] - 0s 348us/step - loss: 0.0013\n",
      "Epoch 150/150\n",
      "34/34 [==============================] - 0s 264us/step - loss: 0.0013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a414f43d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate test and training data splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=445)\n",
    "\n",
    "# Train the network\n",
    "baseNetwork = base_model()\n",
    "\n",
    "baseNetwork.fit(X_train, y_train, epochs=150, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the testing set: 0.0031954790465533733\n"
     ]
    }
   ],
   "source": [
    "test_loss = baseNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "print('The loss on the testing set:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss using sklearn method: 0.2801253106965247\n"
     ]
    }
   ],
   "source": [
    "# Compare to exact predictor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n",
    "pred_values = reg.predict(X_test)\n",
    "mse = np.sqrt( np.mean((pred_values - y_test) ** 2 ) )\n",
    "print('The loss using sklearn method:', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single hidden layer\n",
    "As a first pass at a more complex topology, I will use a network with a single hidden layer smaller than the input data (which is 17 dimensional). The hidden layer will arbitrarily be chosen to be of size 8.\n",
    "\n",
    "The motivation for this structure is as follows:\n",
    "1. The initial attempt using linear regression considered all parameters individually. However, the effectiveness of these measures are not independent of each other (eg. high testing rates will not do anything without some quarantine measures). \n",
    "2. The hidden layer might be able to capture in essence the idea that the effectiveness is dependent on several features together.\n",
    "3. The hidden layer could be expected to be smaller than the input since not all combinations are significant there are many combinations of parameters avaialble.\n",
    "\n",
    "I will use a sigmoid activation function in the hidden layer and again the mean squared error loss function. The sigmoid activation seems to be a decent choice since the hope is that the hidden layer will capture some essence of combinations of the features, so the normalizing effect should help the output neuron be less biased (not really sure if this is a valid argument). Additonally, the data is not too extreme in value and the dataset is relatively small so it shouldn't have too many convergence issues.\n",
    "\n",
    "As a first attempt, I will use the Adam optimizer because it is popular and I don't quite understand all the parameters available in SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thin_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=17, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    # Initialize weights\n",
    "    model.layers[0].set_weights([np.zeros([17,8])+0.01, np.zeros([8,])])\n",
    "    model.layers[1].set_weights([np.zeros([8,1])+0.01, np.array([0.00])])\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "34/34 [==============================] - 1s 19ms/step - loss: 0.0136\n",
      "Epoch 2/150\n",
      "34/34 [==============================] - 0s 295us/step - loss: 0.0037\n",
      "Epoch 3/150\n",
      "34/34 [==============================] - 0s 280us/step - loss: 0.0011\n",
      "Epoch 4/150\n",
      "34/34 [==============================] - 0s 340us/step - loss: 0.0016\n",
      "Epoch 5/150\n",
      "34/34 [==============================] - 0s 354us/step - loss: 0.0018\n",
      "Epoch 6/150\n",
      "34/34 [==============================] - 0s 449us/step - loss: 0.0014\n",
      "Epoch 7/150\n",
      "34/34 [==============================] - 0s 358us/step - loss: 0.0013\n",
      "Epoch 8/150\n",
      "34/34 [==============================] - 0s 313us/step - loss: 0.0012\n",
      "Epoch 9/150\n",
      "34/34 [==============================] - 0s 362us/step - loss: 0.0012\n",
      "Epoch 10/150\n",
      "34/34 [==============================] - 0s 325us/step - loss: 0.0012\n",
      "Epoch 11/150\n",
      "34/34 [==============================] - 0s 275us/step - loss: 0.0012\n",
      "Epoch 12/150\n",
      "34/34 [==============================] - 0s 333us/step - loss: 0.0012\n",
      "Epoch 13/150\n",
      "34/34 [==============================] - 0s 296us/step - loss: 0.0012\n",
      "Epoch 14/150\n",
      "34/34 [==============================] - 0s 257us/step - loss: 0.0012\n",
      "Epoch 15/150\n",
      "34/34 [==============================] - 0s 332us/step - loss: 0.0012\n",
      "Epoch 16/150\n",
      "34/34 [==============================] - 0s 359us/step - loss: 0.0012\n",
      "Epoch 17/150\n",
      "34/34 [==============================] - 0s 368us/step - loss: 0.0012\n",
      "Epoch 18/150\n",
      "34/34 [==============================] - 0s 401us/step - loss: 0.0012\n",
      "Epoch 19/150\n",
      "34/34 [==============================] - 0s 339us/step - loss: 0.0012\n",
      "Epoch 20/150\n",
      "34/34 [==============================] - 0s 388us/step - loss: 0.0011\n",
      "Epoch 21/150\n",
      "34/34 [==============================] - 0s 380us/step - loss: 0.0010\n",
      "Epoch 22/150\n",
      "34/34 [==============================] - 0s 370us/step - loss: 8.9296e-04\n",
      "Epoch 23/150\n",
      "34/34 [==============================] - 0s 315us/step - loss: 8.6782e-04\n",
      "Epoch 24/150\n",
      "34/34 [==============================] - 0s 349us/step - loss: 9.2839e-04\n",
      "Epoch 25/150\n",
      "34/34 [==============================] - 0s 432us/step - loss: 8.7505e-04\n",
      "Epoch 26/150\n",
      "34/34 [==============================] - 0s 464us/step - loss: 9.0227e-04\n",
      "Epoch 27/150\n",
      "34/34 [==============================] - 0s 399us/step - loss: 8.5535e-04\n",
      "Epoch 28/150\n",
      "34/34 [==============================] - 0s 342us/step - loss: 8.5625e-04\n",
      "Epoch 29/150\n",
      "34/34 [==============================] - 0s 389us/step - loss: 8.5991e-04\n",
      "Epoch 30/150\n",
      "34/34 [==============================] - 0s 319us/step - loss: 8.4674e-04\n",
      "Epoch 31/150\n",
      "34/34 [==============================] - 0s 372us/step - loss: 8.5164e-04\n",
      "Epoch 32/150\n",
      "34/34 [==============================] - 0s 351us/step - loss: 8.4583e-04\n",
      "Epoch 33/150\n",
      "34/34 [==============================] - 0s 335us/step - loss: 8.3888e-04\n",
      "Epoch 34/150\n",
      "34/34 [==============================] - 0s 354us/step - loss: 8.3742e-04\n",
      "Epoch 35/150\n",
      "34/34 [==============================] - 0s 362us/step - loss: 8.3196e-04\n",
      "Epoch 36/150\n",
      "34/34 [==============================] - 0s 426us/step - loss: 8.2830e-04\n",
      "Epoch 37/150\n",
      "34/34 [==============================] - 0s 355us/step - loss: 8.2523e-04\n",
      "Epoch 38/150\n",
      "34/34 [==============================] - 0s 321us/step - loss: 8.2038e-04\n",
      "Epoch 39/150\n",
      "34/34 [==============================] - 0s 289us/step - loss: 8.1643e-04\n",
      "Epoch 40/150\n",
      "34/34 [==============================] - 0s 366us/step - loss: 8.1206e-04\n",
      "Epoch 41/150\n",
      "34/34 [==============================] - 0s 334us/step - loss: 8.0743e-04\n",
      "Epoch 42/150\n",
      "34/34 [==============================] - 0s 354us/step - loss: 8.0326e-04\n",
      "Epoch 43/150\n",
      "34/34 [==============================] - 0s 300us/step - loss: 7.9893e-04\n",
      "Epoch 44/150\n",
      "34/34 [==============================] - 0s 323us/step - loss: 7.9494e-04\n",
      "Epoch 45/150\n",
      "34/34 [==============================] - 0s 322us/step - loss: 7.9129e-04\n",
      "Epoch 46/150\n",
      "34/34 [==============================] - 0s 315us/step - loss: 7.8790e-04\n",
      "Epoch 47/150\n",
      "34/34 [==============================] - 0s 473us/step - loss: 7.8491e-04\n",
      "Epoch 48/150\n",
      "34/34 [==============================] - 0s 378us/step - loss: 7.8212e-04\n",
      "Epoch 49/150\n",
      "34/34 [==============================] - 0s 369us/step - loss: 7.7956e-04\n",
      "Epoch 50/150\n",
      "34/34 [==============================] - 0s 382us/step - loss: 7.7715e-04\n",
      "Epoch 51/150\n",
      "34/34 [==============================] - 0s 352us/step - loss: 7.7491e-04\n",
      "Epoch 52/150\n",
      "34/34 [==============================] - 0s 341us/step - loss: 7.7282e-04\n",
      "Epoch 53/150\n",
      "34/34 [==============================] - 0s 382us/step - loss: 7.7084e-04\n",
      "Epoch 54/150\n",
      "34/34 [==============================] - 0s 363us/step - loss: 7.6901e-04\n",
      "Epoch 55/150\n",
      "34/34 [==============================] - 0s 364us/step - loss: 7.6727e-04\n",
      "Epoch 56/150\n",
      "34/34 [==============================] - 0s 327us/step - loss: 7.6563e-04\n",
      "Epoch 57/150\n",
      "34/34 [==============================] - 0s 317us/step - loss: 7.6407e-04\n",
      "Epoch 58/150\n",
      "34/34 [==============================] - 0s 387us/step - loss: 7.6257e-04\n",
      "Epoch 59/150\n",
      "34/34 [==============================] - 0s 388us/step - loss: 7.6114e-04\n",
      "Epoch 60/150\n",
      "34/34 [==============================] - 0s 404us/step - loss: 7.5975e-04\n",
      "Epoch 61/150\n",
      "34/34 [==============================] - 0s 435us/step - loss: 7.5841e-04\n",
      "Epoch 62/150\n",
      "34/34 [==============================] - 0s 291us/step - loss: 7.5714e-04\n",
      "Epoch 63/150\n",
      "34/34 [==============================] - 0s 305us/step - loss: 7.5584e-04\n",
      "Epoch 64/150\n",
      "34/34 [==============================] - 0s 301us/step - loss: 7.5461e-04\n",
      "Epoch 65/150\n",
      "34/34 [==============================] - 0s 290us/step - loss: 7.5342e-04\n",
      "Epoch 66/150\n",
      "34/34 [==============================] - 0s 317us/step - loss: 7.5222e-04\n",
      "Epoch 67/150\n",
      "34/34 [==============================] - 0s 324us/step - loss: 7.5108e-04\n",
      "Epoch 68/150\n",
      "34/34 [==============================] - 0s 286us/step - loss: 7.4994e-04\n",
      "Epoch 69/150\n",
      "34/34 [==============================] - 0s 294us/step - loss: 7.4882e-04\n",
      "Epoch 70/150\n",
      "34/34 [==============================] - 0s 290us/step - loss: 7.4773e-04\n",
      "Epoch 71/150\n",
      "34/34 [==============================] - 0s 291us/step - loss: 7.4664e-04\n",
      "Epoch 72/150\n",
      "34/34 [==============================] - 0s 345us/step - loss: 7.4558e-04\n",
      "Epoch 73/150\n",
      "34/34 [==============================] - 0s 334us/step - loss: 7.4453e-04\n",
      "Epoch 74/150\n",
      "34/34 [==============================] - 0s 292us/step - loss: 7.4349e-04\n",
      "Epoch 75/150\n",
      "34/34 [==============================] - 0s 298us/step - loss: 7.4246e-04\n",
      "Epoch 76/150\n",
      "34/34 [==============================] - 0s 320us/step - loss: 7.4145e-04\n",
      "Epoch 77/150\n",
      "34/34 [==============================] - 0s 304us/step - loss: 7.4045e-04\n",
      "Epoch 78/150\n",
      "34/34 [==============================] - 0s 355us/step - loss: 7.3946e-04\n",
      "Epoch 79/150\n",
      "34/34 [==============================] - 0s 333us/step - loss: 7.3847e-04\n",
      "Epoch 80/150\n",
      "34/34 [==============================] - 0s 298us/step - loss: 7.3750e-04\n",
      "Epoch 81/150\n",
      "34/34 [==============================] - 0s 283us/step - loss: 7.3654e-04\n",
      "Epoch 82/150\n",
      "34/34 [==============================] - 0s 324us/step - loss: 7.3558e-04\n",
      "Epoch 83/150\n",
      "34/34 [==============================] - 0s 277us/step - loss: 7.3464e-04\n",
      "Epoch 84/150\n",
      "34/34 [==============================] - 0s 405us/step - loss: 7.3370e-04\n",
      "Epoch 85/150\n",
      "34/34 [==============================] - 0s 363us/step - loss: 7.3277e-04\n",
      "Epoch 86/150\n",
      "34/34 [==============================] - 0s 328us/step - loss: 7.3184e-04\n",
      "Epoch 87/150\n",
      "34/34 [==============================] - 0s 336us/step - loss: 7.3093e-04\n",
      "Epoch 88/150\n",
      "34/34 [==============================] - 0s 288us/step - loss: 7.3002e-04\n",
      "Epoch 89/150\n",
      "34/34 [==============================] - 0s 338us/step - loss: 7.2912e-04\n",
      "Epoch 90/150\n",
      "34/34 [==============================] - 0s 347us/step - loss: 7.2822e-04\n",
      "Epoch 91/150\n",
      "34/34 [==============================] - 0s 302us/step - loss: 7.2733e-04\n",
      "Epoch 92/150\n",
      "34/34 [==============================] - 0s 302us/step - loss: 7.2644e-04\n",
      "Epoch 93/150\n",
      "34/34 [==============================] - 0s 335us/step - loss: 7.2557e-04\n",
      "Epoch 94/150\n",
      "34/34 [==============================] - 0s 271us/step - loss: 7.2469e-04\n",
      "Epoch 95/150\n",
      "34/34 [==============================] - 0s 370us/step - loss: 7.2383e-04\n",
      "Epoch 96/150\n",
      "34/34 [==============================] - 0s 335us/step - loss: 7.2291e-04\n",
      "Epoch 97/150\n",
      "34/34 [==============================] - 0s 287us/step - loss: 7.2215e-04\n",
      "Epoch 98/150\n",
      "34/34 [==============================] - 0s 293us/step - loss: 7.2124e-04\n",
      "Epoch 99/150\n",
      "34/34 [==============================] - 0s 344us/step - loss: 7.2040e-04\n",
      "Epoch 100/150\n",
      "34/34 [==============================] - 0s 293us/step - loss: 7.1956e-04\n",
      "Epoch 101/150\n",
      "34/34 [==============================] - 0s 305us/step - loss: 7.1871e-04\n",
      "Epoch 102/150\n",
      "34/34 [==============================] - 0s 350us/step - loss: 7.1791e-04\n",
      "Epoch 103/150\n",
      "34/34 [==============================] - 0s 282us/step - loss: 7.1706e-04\n",
      "Epoch 104/150\n",
      "34/34 [==============================] - 0s 279us/step - loss: 7.1625e-04\n",
      "Epoch 105/150\n",
      "34/34 [==============================] - 0s 344us/step - loss: 7.1541e-04\n",
      "Epoch 106/150\n",
      "34/34 [==============================] - 0s 311us/step - loss: 7.1462e-04\n",
      "Epoch 107/150\n",
      "34/34 [==============================] - 0s 321us/step - loss: 7.1379e-04\n",
      "Epoch 108/150\n",
      "34/34 [==============================] - 0s 460us/step - loss: 7.1298e-04\n",
      "Epoch 109/150\n",
      "34/34 [==============================] - 0s 329us/step - loss: 7.1218e-04\n",
      "Epoch 110/150\n",
      "34/34 [==============================] - 0s 323us/step - loss: 7.1137e-04\n",
      "Epoch 111/150\n",
      "34/34 [==============================] - 0s 281us/step - loss: 7.1058e-04\n",
      "Epoch 112/150\n",
      "34/34 [==============================] - 0s 352us/step - loss: 7.0979e-04\n",
      "Epoch 113/150\n",
      "34/34 [==============================] - 0s 366us/step - loss: 7.0896e-04\n",
      "Epoch 114/150\n",
      "34/34 [==============================] - 0s 283us/step - loss: 7.0825e-04\n",
      "Epoch 115/150\n",
      "34/34 [==============================] - 0s 286us/step - loss: 7.0741e-04\n",
      "Epoch 116/150\n",
      "34/34 [==============================] - 0s 322us/step - loss: 7.0663e-04\n",
      "Epoch 117/150\n",
      "34/34 [==============================] - 0s 307us/step - loss: 7.0589e-04\n",
      "Epoch 118/150\n",
      "34/34 [==============================] - 0s 332us/step - loss: 7.0509e-04\n",
      "Epoch 119/150\n",
      "34/34 [==============================] - 0s 382us/step - loss: 7.0434e-04\n",
      "Epoch 120/150\n",
      "34/34 [==============================] - 0s 285us/step - loss: 7.0356e-04\n",
      "Epoch 121/150\n",
      "34/34 [==============================] - 0s 266us/step - loss: 7.0280e-04\n",
      "Epoch 122/150\n",
      "34/34 [==============================] - 0s 362us/step - loss: 7.0204e-04\n",
      "Epoch 123/150\n",
      "34/34 [==============================] - 0s 332us/step - loss: 7.0128e-04\n",
      "Epoch 124/150\n",
      "34/34 [==============================] - 0s 262us/step - loss: 7.0053e-04\n",
      "Epoch 125/150\n",
      "34/34 [==============================] - 0s 370us/step - loss: 6.9977e-04\n",
      "Epoch 126/150\n",
      "34/34 [==============================] - 0s 284us/step - loss: 6.9902e-04\n",
      "Epoch 127/150\n",
      "34/34 [==============================] - 0s 262us/step - loss: 6.9828e-04\n",
      "Epoch 128/150\n",
      "34/34 [==============================] - 0s 394us/step - loss: 6.9753e-04\n",
      "Epoch 129/150\n",
      "34/34 [==============================] - 0s 425us/step - loss: 6.9679e-04\n",
      "Epoch 130/150\n",
      "34/34 [==============================] - 0s 451us/step - loss: 6.9605e-04\n",
      "Epoch 131/150\n",
      "34/34 [==============================] - 0s 279us/step - loss: 6.9534e-04\n",
      "Epoch 132/150\n",
      "34/34 [==============================] - 0s 271us/step - loss: 6.9455e-04\n",
      "Epoch 133/150\n",
      "34/34 [==============================] - 0s 330us/step - loss: 6.9386e-04\n",
      "Epoch 134/150\n",
      "34/34 [==============================] - 0s 338us/step - loss: 6.9312e-04\n",
      "Epoch 135/150\n",
      "34/34 [==============================] - 0s 352us/step - loss: 6.9239e-04\n",
      "Epoch 136/150\n",
      "34/34 [==============================] - 0s 260us/step - loss: 6.9168e-04\n",
      "Epoch 137/150\n",
      "34/34 [==============================] - 0s 329us/step - loss: 6.9095e-04\n",
      "Epoch 138/150\n",
      "34/34 [==============================] - 0s 301us/step - loss: 6.9023e-04\n",
      "Epoch 139/150\n",
      "34/34 [==============================] - 0s 331us/step - loss: 6.8951e-04\n",
      "Epoch 140/150\n",
      "34/34 [==============================] - 0s 341us/step - loss: 6.8880e-04\n",
      "Epoch 141/150\n",
      "34/34 [==============================] - 0s 323us/step - loss: 6.8809e-04\n",
      "Epoch 142/150\n",
      "34/34 [==============================] - 0s 306us/step - loss: 6.8737e-04\n",
      "Epoch 143/150\n",
      "34/34 [==============================] - 0s 296us/step - loss: 6.8667e-04\n",
      "Epoch 144/150\n",
      "34/34 [==============================] - 0s 296us/step - loss: 6.8596e-04\n",
      "Epoch 145/150\n",
      "34/34 [==============================] - 0s 285us/step - loss: 6.8526e-04\n",
      "Epoch 146/150\n",
      "34/34 [==============================] - 0s 320us/step - loss: 6.8456e-04\n",
      "Epoch 147/150\n",
      "34/34 [==============================] - 0s 290us/step - loss: 6.8386e-04\n",
      "Epoch 148/150\n",
      "34/34 [==============================] - 0s 282us/step - loss: 6.8316e-04\n",
      "Epoch 149/150\n",
      "34/34 [==============================] - 0s 275us/step - loss: 6.8246e-04\n",
      "Epoch 150/150\n",
      "34/34 [==============================] - 0s 279us/step - loss: 6.8177e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4195ce90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thinNetwork = thin_model()\n",
    "\n",
    "thinNetwork.fit(X_train, y_train, epochs=150, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the testing set: 0.0014621663140133023\n"
     ]
    }
   ],
   "source": [
    "test_loss = thinNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "print('The loss on the testing set:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, at least for this particular training and testing set, the single layer thing network did a significantly better job on predicting the output than the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two hidden layers\n",
    "Adding additional complexity, I will add a second hidden layer, also of size 8.\n",
    "\n",
    "The hope was that the first hidden layer in the previous attempt might capture the essence of \"combinations\" of the parameters being significant. However, not all combinations are trivial. For instance, high early testing rates might be significant only if quarantine measures are enacted, but quarantine measures could be enacted without high testing rates and still be effective. Additional complexities like this might hopefully be captured by the addition of a second hidden layer.\n",
    "\n",
    "I will use the same parameters for the second layer as well (at least initially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=17, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(8, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    model.layers[0].set_weights([np.zeros([17,8])+0.01, np.zeros([8,])])\n",
    "    model.layers[1].set_weights([np.zeros([8,8])+0.01, np.zeros([8,])])\n",
    "    model.layers[2].set_weights([np.zeros([8,1])+0.01, np.array([0.00])])\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "34/34 [==============================] - 1s 26ms/step - loss: 0.0262\n",
      "Epoch 2/150\n",
      "34/34 [==============================] - 0s 366us/step - loss: 0.0155\n",
      "Epoch 3/150\n",
      "34/34 [==============================] - 0s 363us/step - loss: 0.0078\n",
      "Epoch 4/150\n",
      "34/34 [==============================] - 0s 358us/step - loss: 0.0033\n",
      "Epoch 5/150\n",
      "34/34 [==============================] - 0s 355us/step - loss: 0.0014\n",
      "Epoch 6/150\n",
      "34/34 [==============================] - 0s 385us/step - loss: 0.0011\n",
      "Epoch 7/150\n",
      "34/34 [==============================] - 0s 385us/step - loss: 0.0013\n",
      "Epoch 8/150\n",
      "34/34 [==============================] - 0s 320us/step - loss: 0.0014\n",
      "Epoch 9/150\n",
      "34/34 [==============================] - 0s 388us/step - loss: 0.0013\n",
      "Epoch 10/150\n",
      "34/34 [==============================] - 0s 373us/step - loss: 0.0012\n",
      "Epoch 11/150\n",
      "34/34 [==============================] - 0s 372us/step - loss: 0.0012\n",
      "Epoch 12/150\n",
      "34/34 [==============================] - 0s 426us/step - loss: 0.0012\n",
      "Epoch 13/150\n",
      "34/34 [==============================] - 0s 405us/step - loss: 0.0012\n",
      "Epoch 14/150\n",
      "34/34 [==============================] - 0s 361us/step - loss: 0.0012\n",
      "Epoch 15/150\n",
      "34/34 [==============================] - 0s 349us/step - loss: 0.0012\n",
      "Epoch 16/150\n",
      "34/34 [==============================] - 0s 414us/step - loss: 0.0012\n",
      "Epoch 17/150\n",
      "34/34 [==============================] - 0s 410us/step - loss: 0.0012\n",
      "Epoch 18/150\n",
      "34/34 [==============================] - 0s 427us/step - loss: 0.0012\n",
      "Epoch 19/150\n",
      "34/34 [==============================] - 0s 388us/step - loss: 0.0012\n",
      "Epoch 20/150\n",
      "34/34 [==============================] - 0s 427us/step - loss: 0.0012\n",
      "Epoch 21/150\n",
      "34/34 [==============================] - 0s 373us/step - loss: 0.0012\n",
      "Epoch 22/150\n",
      "34/34 [==============================] - 0s 403us/step - loss: 0.0012\n",
      "Epoch 23/150\n",
      "34/34 [==============================] - 0s 387us/step - loss: 0.0012\n",
      "Epoch 24/150\n",
      "34/34 [==============================] - 0s 371us/step - loss: 0.0012\n",
      "Epoch 25/150\n",
      "34/34 [==============================] - 0s 393us/step - loss: 0.0011\n",
      "Epoch 26/150\n",
      "34/34 [==============================] - 0s 407us/step - loss: 9.6813e-04\n",
      "Epoch 27/150\n",
      "34/34 [==============================] - 0s 414us/step - loss: 9.2937e-04\n",
      "Epoch 28/150\n",
      "34/34 [==============================] - 0s 385us/step - loss: 9.3645e-04\n",
      "Epoch 29/150\n",
      "34/34 [==============================] - 0s 360us/step - loss: 9.3028e-04\n",
      "Epoch 30/150\n",
      "34/34 [==============================] - 0s 461us/step - loss: 9.1563e-04\n",
      "Epoch 31/150\n",
      "34/34 [==============================] - 0s 572us/step - loss: 9.0665e-04\n",
      "Epoch 32/150\n",
      "34/34 [==============================] - 0s 367us/step - loss: 9.0404e-04\n",
      "Epoch 33/150\n",
      "34/34 [==============================] - 0s 369us/step - loss: 9.0164e-04\n",
      "Epoch 34/150\n",
      "34/34 [==============================] - 0s 421us/step - loss: 8.9726e-04\n",
      "Epoch 35/150\n",
      "34/34 [==============================] - 0s 424us/step - loss: 8.9222e-04\n",
      "Epoch 36/150\n",
      "34/34 [==============================] - 0s 419us/step - loss: 8.8794e-04\n",
      "Epoch 37/150\n",
      "34/34 [==============================] - 0s 404us/step - loss: 8.8463e-04\n",
      "Epoch 38/150\n",
      "34/34 [==============================] - 0s 422us/step - loss: 8.8171e-04\n",
      "Epoch 39/150\n",
      "34/34 [==============================] - 0s 391us/step - loss: 8.7875e-04\n",
      "Epoch 40/150\n",
      "34/34 [==============================] - 0s 384us/step - loss: 8.7577e-04\n",
      "Epoch 41/150\n",
      "34/34 [==============================] - 0s 410us/step - loss: 8.7294e-04\n",
      "Epoch 42/150\n",
      "34/34 [==============================] - 0s 383us/step - loss: 8.7031e-04\n",
      "Epoch 43/150\n",
      "34/34 [==============================] - 0s 381us/step - loss: 8.6787e-04\n",
      "Epoch 44/150\n",
      "34/34 [==============================] - 0s 375us/step - loss: 8.6553e-04\n",
      "Epoch 45/150\n",
      "34/34 [==============================] - 0s 374us/step - loss: 8.6326e-04\n",
      "Epoch 46/150\n",
      "34/34 [==============================] - 0s 411us/step - loss: 8.6108e-04\n",
      "Epoch 47/150\n",
      "34/34 [==============================] - 0s 367us/step - loss: 8.5903e-04\n",
      "Epoch 48/150\n",
      "34/34 [==============================] - 0s 426us/step - loss: 8.5708e-04\n",
      "Epoch 49/150\n",
      "34/34 [==============================] - 0s 422us/step - loss: 8.5523e-04\n",
      "Epoch 50/150\n",
      "34/34 [==============================] - 0s 534us/step - loss: 8.5346e-04\n",
      "Epoch 51/150\n",
      "34/34 [==============================] - 0s 384us/step - loss: 8.5177e-04\n",
      "Epoch 52/150\n",
      "34/34 [==============================] - 0s 385us/step - loss: 8.5017e-04\n",
      "Epoch 53/150\n",
      "34/34 [==============================] - 0s 363us/step - loss: 8.4865e-04\n",
      "Epoch 54/150\n",
      "34/34 [==============================] - 0s 411us/step - loss: 8.4720e-04\n",
      "Epoch 55/150\n",
      "34/34 [==============================] - 0s 390us/step - loss: 8.4583e-04\n",
      "Epoch 56/150\n",
      "34/34 [==============================] - 0s 339us/step - loss: 8.4451e-04\n",
      "Epoch 57/150\n",
      "34/34 [==============================] - 0s 370us/step - loss: 8.4325e-04\n",
      "Epoch 58/150\n",
      "34/34 [==============================] - 0s 360us/step - loss: 8.4206e-04\n",
      "Epoch 59/150\n",
      "34/34 [==============================] - 0s 417us/step - loss: 8.4093e-04\n",
      "Epoch 60/150\n",
      "34/34 [==============================] - 0s 413us/step - loss: 8.3985e-04\n",
      "Epoch 61/150\n",
      "34/34 [==============================] - 0s 368us/step - loss: 8.3883e-04\n",
      "Epoch 62/150\n",
      "34/34 [==============================] - 0s 346us/step - loss: 8.3785e-04\n",
      "Epoch 63/150\n",
      "34/34 [==============================] - 0s 329us/step - loss: 8.3693e-04\n",
      "Epoch 64/150\n",
      "34/34 [==============================] - 0s 362us/step - loss: 8.3604e-04\n",
      "Epoch 65/150\n",
      "34/34 [==============================] - 0s 379us/step - loss: 8.3520e-04\n",
      "Epoch 66/150\n",
      "34/34 [==============================] - 0s 326us/step - loss: 8.3440e-04\n",
      "Epoch 67/150\n",
      "34/34 [==============================] - 0s 321us/step - loss: 8.3364e-04\n",
      "Epoch 68/150\n",
      "34/34 [==============================] - 0s 375us/step - loss: 8.3292e-04\n",
      "Epoch 69/150\n",
      "34/34 [==============================] - 0s 450us/step - loss: 8.3223e-04\n",
      "Epoch 70/150\n",
      "34/34 [==============================] - 0s 377us/step - loss: 8.3157e-04\n",
      "Epoch 71/150\n",
      "34/34 [==============================] - 0s 344us/step - loss: 8.3095e-04\n",
      "Epoch 72/150\n",
      "34/34 [==============================] - 0s 334us/step - loss: 8.3034e-04\n",
      "Epoch 73/150\n",
      "34/34 [==============================] - 0s 357us/step - loss: 8.2977e-04\n",
      "Epoch 74/150\n",
      "34/34 [==============================] - 0s 399us/step - loss: 8.2923e-04\n",
      "Epoch 75/150\n",
      "34/34 [==============================] - 0s 387us/step - loss: 8.2871e-04\n",
      "Epoch 76/150\n",
      "34/34 [==============================] - 0s 328us/step - loss: 8.2822e-04\n",
      "Epoch 77/150\n",
      "34/34 [==============================] - 0s 321us/step - loss: 8.2774e-04\n",
      "Epoch 78/150\n",
      "34/34 [==============================] - 0s 398us/step - loss: 8.2729e-04\n",
      "Epoch 79/150\n",
      "34/34 [==============================] - 0s 412us/step - loss: 8.2686e-04\n",
      "Epoch 80/150\n",
      "34/34 [==============================] - 0s 386us/step - loss: 8.2644e-04\n",
      "Epoch 81/150\n",
      "34/34 [==============================] - 0s 355us/step - loss: 8.2605e-04\n",
      "Epoch 82/150\n",
      "34/34 [==============================] - 0s 321us/step - loss: 8.2567e-04\n",
      "Epoch 83/150\n",
      "34/34 [==============================] - 0s 386us/step - loss: 8.2531e-04\n",
      "Epoch 84/150\n",
      "34/34 [==============================] - 0s 398us/step - loss: 8.2496e-04\n",
      "Epoch 85/150\n",
      "34/34 [==============================] - 0s 369us/step - loss: 8.2462e-04\n",
      "Epoch 86/150\n",
      "34/34 [==============================] - 0s 338us/step - loss: 8.2430e-04\n",
      "Epoch 87/150\n",
      "34/34 [==============================] - 0s 335us/step - loss: 8.2399e-04\n",
      "Epoch 88/150\n",
      "34/34 [==============================] - 0s 391us/step - loss: 8.2369e-04\n",
      "Epoch 89/150\n",
      "34/34 [==============================] - 0s 456us/step - loss: 8.2341e-04\n",
      "Epoch 90/150\n",
      "34/34 [==============================] - 0s 507us/step - loss: 8.2313e-04\n",
      "Epoch 91/150\n",
      "34/34 [==============================] - 0s 356us/step - loss: 8.2287e-04\n",
      "Epoch 92/150\n",
      "34/34 [==============================] - 0s 373us/step - loss: 8.2261e-04\n",
      "Epoch 93/150\n",
      "34/34 [==============================] - 0s 389us/step - loss: 8.2236e-04\n",
      "Epoch 94/150\n",
      "34/34 [==============================] - 0s 353us/step - loss: 8.2212e-04\n",
      "Epoch 95/150\n",
      "34/34 [==============================] - 0s 383us/step - loss: 8.2189e-04\n",
      "Epoch 96/150\n",
      "34/34 [==============================] - 0s 368us/step - loss: 8.2167e-04\n",
      "Epoch 97/150\n",
      "34/34 [==============================] - 0s 368us/step - loss: 8.2145e-04\n",
      "Epoch 98/150\n",
      "34/34 [==============================] - 0s 421us/step - loss: 8.2124e-04\n",
      "Epoch 99/150\n",
      "34/34 [==============================] - 0s 378us/step - loss: 8.2104e-04\n",
      "Epoch 100/150\n",
      "34/34 [==============================] - 0s 337us/step - loss: 8.2084e-04\n",
      "Epoch 101/150\n",
      "34/34 [==============================] - 0s 342us/step - loss: 8.2065e-04\n",
      "Epoch 102/150\n",
      "34/34 [==============================] - 0s 335us/step - loss: 8.2046e-04\n",
      "Epoch 103/150\n",
      "34/34 [==============================] - 0s 359us/step - loss: 8.2028e-04\n",
      "Epoch 104/150\n",
      "34/34 [==============================] - 0s 340us/step - loss: 8.2010e-04\n",
      "Epoch 105/150\n",
      "34/34 [==============================] - 0s 383us/step - loss: 8.1993e-04\n",
      "Epoch 106/150\n",
      "34/34 [==============================] - 0s 335us/step - loss: 8.1976e-04\n",
      "Epoch 107/150\n",
      "34/34 [==============================] - 0s 351us/step - loss: 8.1960e-04\n",
      "Epoch 108/150\n",
      "34/34 [==============================] - 0s 344us/step - loss: 8.1944e-04\n",
      "Epoch 109/150\n",
      "34/34 [==============================] - 0s 400us/step - loss: 8.1928e-04\n",
      "Epoch 110/150\n",
      "34/34 [==============================] - 0s 439us/step - loss: 8.1913e-04\n",
      "Epoch 111/150\n",
      "34/34 [==============================] - 0s 364us/step - loss: 8.1898e-04\n",
      "Epoch 112/150\n",
      "34/34 [==============================] - 0s 357us/step - loss: 8.1883e-04\n",
      "Epoch 113/150\n",
      "34/34 [==============================] - 0s 363us/step - loss: 8.1869e-04\n",
      "Epoch 114/150\n",
      "34/34 [==============================] - 0s 338us/step - loss: 8.1855e-04\n",
      "Epoch 115/150\n",
      "34/34 [==============================] - 0s 366us/step - loss: 8.1841e-04\n",
      "Epoch 116/150\n",
      "34/34 [==============================] - 0s 325us/step - loss: 8.1828e-04\n",
      "Epoch 117/150\n",
      "34/34 [==============================] - 0s 323us/step - loss: 8.1814e-04\n",
      "Epoch 118/150\n",
      "34/34 [==============================] - 0s 342us/step - loss: 8.1802e-04\n",
      "Epoch 119/150\n",
      "34/34 [==============================] - 0s 380us/step - loss: 8.1789e-04\n",
      "Epoch 120/150\n",
      "34/34 [==============================] - 0s 346us/step - loss: 8.1776e-04\n",
      "Epoch 121/150\n",
      "34/34 [==============================] - 0s 339us/step - loss: 8.1764e-04\n",
      "Epoch 122/150\n",
      "34/34 [==============================] - 0s 321us/step - loss: 8.1752e-04\n",
      "Epoch 123/150\n",
      "34/34 [==============================] - 0s 354us/step - loss: 8.1740e-04\n",
      "Epoch 124/150\n",
      "34/34 [==============================] - 0s 335us/step - loss: 8.1728e-04\n",
      "Epoch 125/150\n",
      "34/34 [==============================] - 0s 328us/step - loss: 8.1717e-04\n",
      "Epoch 126/150\n",
      "34/34 [==============================] - 0s 362us/step - loss: 8.1706e-04\n",
      "Epoch 127/150\n",
      "34/34 [==============================] - 0s 315us/step - loss: 8.1694e-04\n",
      "Epoch 128/150\n",
      "34/34 [==============================] - 0s 328us/step - loss: 8.1684e-04\n",
      "Epoch 129/150\n",
      "34/34 [==============================] - 0s 332us/step - loss: 8.1673e-04\n",
      "Epoch 130/150\n",
      "34/34 [==============================] - 0s 388us/step - loss: 8.1662e-04\n",
      "Epoch 131/150\n",
      "34/34 [==============================] - 0s 438us/step - loss: 8.1652e-04\n",
      "Epoch 132/150\n",
      "34/34 [==============================] - 0s 327us/step - loss: 8.1641e-04\n",
      "Epoch 133/150\n",
      "34/34 [==============================] - 0s 329us/step - loss: 8.1631e-04\n",
      "Epoch 134/150\n",
      "34/34 [==============================] - 0s 338us/step - loss: 8.1621e-04\n",
      "Epoch 135/150\n",
      "34/34 [==============================] - 0s 353us/step - loss: 8.1611e-04\n",
      "Epoch 136/150\n",
      "34/34 [==============================] - 0s 327us/step - loss: 8.1602e-04\n",
      "Epoch 137/150\n",
      "34/34 [==============================] - 0s 326us/step - loss: 8.1592e-04\n",
      "Epoch 138/150\n",
      "34/34 [==============================] - 0s 320us/step - loss: 8.1583e-04\n",
      "Epoch 139/150\n",
      "34/34 [==============================] - 0s 325us/step - loss: 8.1573e-04\n",
      "Epoch 140/150\n",
      "34/34 [==============================] - 0s 326us/step - loss: 8.1564e-04\n",
      "Epoch 141/150\n",
      "34/34 [==============================] - 0s 357us/step - loss: 8.1555e-04\n",
      "Epoch 142/150\n",
      "34/34 [==============================] - 0s 374us/step - loss: 8.1546e-04\n",
      "Epoch 143/150\n",
      "34/34 [==============================] - 0s 333us/step - loss: 8.1537e-04\n",
      "Epoch 144/150\n",
      "34/34 [==============================] - 0s 316us/step - loss: 8.1528e-04\n",
      "Epoch 145/150\n",
      "34/34 [==============================] - 0s 325us/step - loss: 8.1520e-04\n",
      "Epoch 146/150\n",
      "34/34 [==============================] - 0s 369us/step - loss: 8.1511e-04\n",
      "Epoch 147/150\n",
      "34/34 [==============================] - 0s 358us/step - loss: 8.1503e-04\n",
      "Epoch 148/150\n",
      "34/34 [==============================] - 0s 316us/step - loss: 8.1494e-04\n",
      "Epoch 149/150\n",
      "34/34 [==============================] - 0s 294us/step - loss: 8.1486e-04\n",
      "Epoch 150/150\n",
      "34/34 [==============================] - 0s 308us/step - loss: 8.1478e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a421403d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepNetwork = deep_model()\n",
    "\n",
    "deepNetwork.fit(X_train, y_train, epochs=150, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the testing set: 0.0012646199902519584\n"
     ]
    }
   ],
   "source": [
    "test_loss = deepNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "print('The loss on the testing set:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, the additonal layer did not significantly improve the loss when a signmoid activation function is implemented. Instead trying a RELU activation in the second layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "34/34 [==============================] - 1s 28ms/step - loss: 0.0419\n",
      "Epoch 2/150\n",
      "34/34 [==============================] - 0s 390us/step - loss: 0.0329\n",
      "Epoch 3/150\n",
      "34/34 [==============================] - 0s 367us/step - loss: 0.0227\n",
      "Epoch 4/150\n",
      "34/34 [==============================] - 0s 344us/step - loss: 0.0126\n",
      "Epoch 5/150\n",
      "34/34 [==============================] - 0s 351us/step - loss: 0.0049\n",
      "Epoch 6/150\n",
      "34/34 [==============================] - 0s 414us/step - loss: 0.0014\n",
      "Epoch 7/150\n",
      "34/34 [==============================] - 0s 459us/step - loss: 0.0013\n",
      "Epoch 8/150\n",
      "34/34 [==============================] - 0s 442us/step - loss: 0.0018\n",
      "Epoch 9/150\n",
      "34/34 [==============================] - 0s 404us/step - loss: 0.0015\n",
      "Epoch 10/150\n",
      "34/34 [==============================] - 0s 419us/step - loss: 0.0013\n",
      "Epoch 11/150\n",
      "34/34 [==============================] - 0s 422us/step - loss: 0.0012\n",
      "Epoch 12/150\n",
      "34/34 [==============================] - 0s 429us/step - loss: 0.0012\n",
      "Epoch 13/150\n",
      "34/34 [==============================] - 0s 385us/step - loss: 0.0012\n",
      "Epoch 14/150\n",
      "34/34 [==============================] - 0s 368us/step - loss: 0.0012\n",
      "Epoch 15/150\n",
      "34/34 [==============================] - 0s 424us/step - loss: 0.0012\n",
      "Epoch 16/150\n",
      "34/34 [==============================] - 0s 540us/step - loss: 0.0012\n",
      "Epoch 17/150\n",
      "34/34 [==============================] - 0s 424us/step - loss: 0.0012\n",
      "Epoch 18/150\n",
      "34/34 [==============================] - 0s 396us/step - loss: 0.0012\n",
      "Epoch 19/150\n",
      "34/34 [==============================] - 0s 456us/step - loss: 0.0012\n",
      "Epoch 20/150\n",
      "34/34 [==============================] - 0s 463us/step - loss: 0.0012\n",
      "Epoch 21/150\n",
      "34/34 [==============================] - 0s 448us/step - loss: 0.0012\n",
      "Epoch 22/150\n",
      "34/34 [==============================] - 0s 350us/step - loss: 0.0012\n",
      "Epoch 23/150\n",
      "34/34 [==============================] - 0s 411us/step - loss: 0.0012\n",
      "Epoch 24/150\n",
      "34/34 [==============================] - 0s 422us/step - loss: 0.0012\n",
      "Epoch 25/150\n",
      "34/34 [==============================] - 0s 415us/step - loss: 0.0012\n",
      "Epoch 26/150\n",
      "34/34 [==============================] - 0s 444us/step - loss: 0.0012\n",
      "Epoch 27/150\n",
      "34/34 [==============================] - 0s 403us/step - loss: 0.0012\n",
      "Epoch 28/150\n",
      "34/34 [==============================] - 0s 406us/step - loss: 0.0012\n",
      "Epoch 29/150\n",
      "34/34 [==============================] - 0s 421us/step - loss: 0.0012\n",
      "Epoch 30/150\n",
      "34/34 [==============================] - 0s 443us/step - loss: 0.0012\n",
      "Epoch 31/150\n",
      "34/34 [==============================] - 0s 407us/step - loss: 0.0011\n",
      "Epoch 32/150\n",
      "34/34 [==============================] - 0s 432us/step - loss: 0.0010\n",
      "Epoch 33/150\n",
      "34/34 [==============================] - 0s 484us/step - loss: 9.1526e-04\n",
      "Epoch 34/150\n",
      "34/34 [==============================] - 0s 526us/step - loss: 9.7940e-04\n",
      "Epoch 35/150\n",
      "34/34 [==============================] - 0s 425us/step - loss: 8.5718e-04\n",
      "Epoch 36/150\n",
      "34/34 [==============================] - 0s 428us/step - loss: 8.9560e-04\n",
      "Epoch 37/150\n",
      "34/34 [==============================] - 0s 432us/step - loss: 8.8523e-04\n",
      "Epoch 38/150\n",
      "34/34 [==============================] - 0s 443us/step - loss: 8.5350e-04\n",
      "Epoch 39/150\n",
      "34/34 [==============================] - 0s 393us/step - loss: 8.6825e-04\n",
      "Epoch 40/150\n",
      "34/34 [==============================] - 0s 406us/step - loss: 8.5483e-04\n",
      "Epoch 41/150\n",
      "34/34 [==============================] - 0s 396us/step - loss: 8.4883e-04\n",
      "Epoch 42/150\n",
      "34/34 [==============================] - 0s 462us/step - loss: 8.5150e-04\n",
      "Epoch 43/150\n",
      "34/34 [==============================] - 0s 427us/step - loss: 8.4223e-04\n",
      "Epoch 44/150\n",
      "34/34 [==============================] - 0s 384us/step - loss: 8.3942e-04\n",
      "Epoch 45/150\n",
      "34/34 [==============================] - 0s 455us/step - loss: 8.3651e-04\n",
      "Epoch 46/150\n",
      "34/34 [==============================] - 0s 445us/step - loss: 8.3057e-04\n",
      "Epoch 47/150\n",
      "34/34 [==============================] - 0s 430us/step - loss: 8.2766e-04\n",
      "Epoch 48/150\n",
      "34/34 [==============================] - 0s 398us/step - loss: 8.2339e-04\n",
      "Epoch 49/150\n",
      "34/34 [==============================] - 0s 438us/step - loss: 8.1853e-04\n",
      "Epoch 50/150\n",
      "34/34 [==============================] - 0s 461us/step - loss: 8.1449e-04\n",
      "Epoch 51/150\n",
      "34/34 [==============================] - 0s 522us/step - loss: 8.0953e-04\n",
      "Epoch 52/150\n",
      "34/34 [==============================] - 0s 414us/step - loss: 8.0480e-04\n",
      "Epoch 53/150\n",
      "34/34 [==============================] - 0s 427us/step - loss: 8.0015e-04\n",
      "Epoch 54/150\n",
      "34/34 [==============================] - 0s 405us/step - loss: 7.9565e-04\n",
      "Epoch 55/150\n",
      "34/34 [==============================] - 0s 394us/step - loss: 7.9163e-04\n",
      "Epoch 56/150\n",
      "34/34 [==============================] - 0s 388us/step - loss: 7.8792e-04\n",
      "Epoch 57/150\n",
      "34/34 [==============================] - 0s 431us/step - loss: 7.8465e-04\n",
      "Epoch 58/150\n",
      "34/34 [==============================] - 0s 453us/step - loss: 7.8169e-04\n",
      "Epoch 59/150\n",
      "34/34 [==============================] - 0s 446us/step - loss: 7.7895e-04\n",
      "Epoch 60/150\n",
      "34/34 [==============================] - 0s 387us/step - loss: 7.7643e-04\n",
      "Epoch 61/150\n",
      "34/34 [==============================] - 0s 372us/step - loss: 7.7406e-04\n",
      "Epoch 62/150\n",
      "34/34 [==============================] - 0s 321us/step - loss: 7.7185e-04\n",
      "Epoch 63/150\n",
      "34/34 [==============================] - 0s 370us/step - loss: 7.6983e-04\n",
      "Epoch 64/150\n",
      "34/34 [==============================] - 0s 390us/step - loss: 7.6791e-04\n",
      "Epoch 65/150\n",
      "34/34 [==============================] - 0s 367us/step - loss: 7.6614e-04\n",
      "Epoch 66/150\n",
      "34/34 [==============================] - 0s 346us/step - loss: 7.6446e-04\n",
      "Epoch 67/150\n",
      "34/34 [==============================] - 0s 315us/step - loss: 7.6284e-04\n",
      "Epoch 68/150\n",
      "34/34 [==============================] - 0s 382us/step - loss: 7.6133e-04\n",
      "Epoch 69/150\n",
      "34/34 [==============================] - 0s 455us/step - loss: 7.5985e-04\n",
      "Epoch 70/150\n",
      "34/34 [==============================] - 0s 389us/step - loss: 7.5844e-04\n",
      "Epoch 71/150\n",
      "34/34 [==============================] - 0s 325us/step - loss: 7.5707e-04\n",
      "Epoch 72/150\n",
      "34/34 [==============================] - 0s 299us/step - loss: 7.5572e-04\n",
      "Epoch 73/150\n",
      "34/34 [==============================] - 0s 383us/step - loss: 7.5445e-04\n",
      "Epoch 74/150\n",
      "34/34 [==============================] - 0s 379us/step - loss: 7.5315e-04\n",
      "Epoch 75/150\n",
      "34/34 [==============================] - 0s 407us/step - loss: 7.5194e-04\n",
      "Epoch 76/150\n",
      "34/34 [==============================] - 0s 343us/step - loss: 7.5072e-04\n",
      "Epoch 77/150\n",
      "34/34 [==============================] - 0s 320us/step - loss: 7.4953e-04\n",
      "Epoch 78/150\n",
      "34/34 [==============================] - 0s 384us/step - loss: 7.4837e-04\n",
      "Epoch 79/150\n",
      "34/34 [==============================] - 0s 399us/step - loss: 7.4722e-04\n",
      "Epoch 80/150\n",
      "34/34 [==============================] - 0s 385us/step - loss: 7.4609e-04\n",
      "Epoch 81/150\n",
      "34/34 [==============================] - 0s 335us/step - loss: 7.4497e-04\n",
      "Epoch 82/150\n",
      "34/34 [==============================] - 0s 340us/step - loss: 7.4387e-04\n",
      "Epoch 83/150\n",
      "34/34 [==============================] - 0s 363us/step - loss: 7.4278e-04\n",
      "Epoch 84/150\n",
      "34/34 [==============================] - 0s 368us/step - loss: 7.4172e-04\n",
      "Epoch 85/150\n",
      "34/34 [==============================] - 0s 360us/step - loss: 7.4064e-04\n",
      "Epoch 86/150\n",
      "34/34 [==============================] - 0s 315us/step - loss: 7.3962e-04\n",
      "Epoch 87/150\n",
      "34/34 [==============================] - 0s 324us/step - loss: 7.3857e-04\n",
      "Epoch 88/150\n",
      "34/34 [==============================] - 0s 350us/step - loss: 7.3754e-04\n",
      "Epoch 89/150\n",
      "34/34 [==============================] - 0s 347us/step - loss: 7.3653e-04\n",
      "Epoch 90/150\n",
      "34/34 [==============================] - 0s 466us/step - loss: 7.3552e-04\n",
      "Epoch 91/150\n",
      "34/34 [==============================] - 0s 362us/step - loss: 7.3452e-04\n",
      "Epoch 92/150\n",
      "34/34 [==============================] - 0s 343us/step - loss: 7.3354e-04\n",
      "Epoch 93/150\n",
      "34/34 [==============================] - 0s 367us/step - loss: 7.3253e-04\n",
      "Epoch 94/150\n",
      "34/34 [==============================] - 0s 386us/step - loss: 7.3161e-04\n",
      "Epoch 95/150\n",
      "34/34 [==============================] - 0s 332us/step - loss: 7.3061e-04\n",
      "Epoch 96/150\n",
      "34/34 [==============================] - 0s 371us/step - loss: 7.2966e-04\n",
      "Epoch 97/150\n",
      "34/34 [==============================] - 0s 354us/step - loss: 7.2872e-04\n",
      "Epoch 98/150\n",
      "34/34 [==============================] - 0s 395us/step - loss: 7.2777e-04\n",
      "Epoch 99/150\n",
      "34/34 [==============================] - 0s 366us/step - loss: 7.2684e-04\n",
      "Epoch 100/150\n",
      "34/34 [==============================] - 0s 367us/step - loss: 7.2590e-04\n",
      "Epoch 101/150\n",
      "34/34 [==============================] - 0s 370us/step - loss: 7.2498e-04\n",
      "Epoch 102/150\n",
      "34/34 [==============================] - 0s 352us/step - loss: 7.2407e-04\n",
      "Epoch 103/150\n",
      "34/34 [==============================] - 0s 379us/step - loss: 7.2314e-04\n",
      "Epoch 104/150\n",
      "34/34 [==============================] - 0s 358us/step - loss: 7.2226e-04\n",
      "Epoch 105/150\n",
      "34/34 [==============================] - 0s 312us/step - loss: 7.2134e-04\n",
      "Epoch 106/150\n",
      "34/34 [==============================] - 0s 387us/step - loss: 7.2045e-04\n",
      "Epoch 107/150\n",
      "34/34 [==============================] - 0s 321us/step - loss: 7.1957e-04\n",
      "Epoch 108/150\n",
      "34/34 [==============================] - 0s 291us/step - loss: 7.1868e-04\n",
      "Epoch 109/150\n",
      "34/34 [==============================] - 0s 376us/step - loss: 7.1780e-04\n",
      "Epoch 110/150\n",
      "34/34 [==============================] - 0s 421us/step - loss: 7.1693e-04\n",
      "Epoch 111/150\n",
      "34/34 [==============================] - 0s 446us/step - loss: 7.1605e-04\n",
      "Epoch 112/150\n",
      "34/34 [==============================] - 0s 363us/step - loss: 7.1519e-04\n",
      "Epoch 113/150\n",
      "34/34 [==============================] - 0s 371us/step - loss: 7.1433e-04\n",
      "Epoch 114/150\n",
      "34/34 [==============================] - 0s 325us/step - loss: 7.1348e-04\n",
      "Epoch 115/150\n",
      "34/34 [==============================] - 0s 360us/step - loss: 7.1261e-04\n",
      "Epoch 116/150\n",
      "34/34 [==============================] - 0s 375us/step - loss: 7.1178e-04\n",
      "Epoch 117/150\n",
      "34/34 [==============================] - 0s 353us/step - loss: 7.1092e-04\n",
      "Epoch 118/150\n",
      "34/34 [==============================] - 0s 309us/step - loss: 7.1009e-04\n",
      "Epoch 119/150\n",
      "34/34 [==============================] - 0s 370us/step - loss: 7.0925e-04\n",
      "Epoch 120/150\n",
      "34/34 [==============================] - 0s 432us/step - loss: 7.0841e-04\n",
      "Epoch 121/150\n",
      "34/34 [==============================] - 0s 384us/step - loss: 7.0760e-04\n",
      "Epoch 122/150\n",
      "34/34 [==============================] - 0s 339us/step - loss: 7.0677e-04\n",
      "Epoch 123/150\n",
      "34/34 [==============================] - 0s 313us/step - loss: 7.0595e-04\n",
      "Epoch 124/150\n",
      "34/34 [==============================] - 0s 356us/step - loss: 7.0513e-04\n",
      "Epoch 125/150\n",
      "34/34 [==============================] - 0s 357us/step - loss: 7.0432e-04\n",
      "Epoch 126/150\n",
      "34/34 [==============================] - 0s 323us/step - loss: 7.0351e-04\n",
      "Epoch 127/150\n",
      "34/34 [==============================] - 0s 300us/step - loss: 7.0270e-04\n",
      "Epoch 128/150\n",
      "34/34 [==============================] - 0s 306us/step - loss: 7.0190e-04\n",
      "Epoch 129/150\n",
      "34/34 [==============================] - 0s 338us/step - loss: 7.0110e-04\n",
      "Epoch 130/150\n",
      "34/34 [==============================] - 0s 362us/step - loss: 7.0030e-04\n",
      "Epoch 131/150\n",
      "34/34 [==============================] - 0s 459us/step - loss: 6.9950e-04\n",
      "Epoch 132/150\n",
      "34/34 [==============================] - 0s 375us/step - loss: 6.9871e-04\n",
      "Epoch 133/150\n",
      "34/34 [==============================] - 0s 343us/step - loss: 6.9792e-04\n",
      "Epoch 134/150\n",
      "34/34 [==============================] - 0s 363us/step - loss: 6.9714e-04\n",
      "Epoch 135/150\n",
      "34/34 [==============================] - 0s 374us/step - loss: 6.9636e-04\n",
      "Epoch 136/150\n",
      "34/34 [==============================] - 0s 352us/step - loss: 6.9557e-04\n",
      "Epoch 137/150\n",
      "34/34 [==============================] - 0s 392us/step - loss: 6.9481e-04\n",
      "Epoch 138/150\n",
      "34/34 [==============================] - 0s 321us/step - loss: 6.9402e-04\n",
      "Epoch 139/150\n",
      "34/34 [==============================] - 0s 354us/step - loss: 6.9327e-04\n",
      "Epoch 140/150\n",
      "34/34 [==============================] - 0s 334us/step - loss: 6.9248e-04\n",
      "Epoch 141/150\n",
      "34/34 [==============================] - 0s 411us/step - loss: 6.9173e-04\n",
      "Epoch 142/150\n",
      "34/34 [==============================] - 0s 386us/step - loss: 6.9096e-04\n",
      "Epoch 143/150\n",
      "34/34 [==============================] - 0s 323us/step - loss: 6.9020e-04\n",
      "Epoch 144/150\n",
      "34/34 [==============================] - 0s 361us/step - loss: 6.8945e-04\n",
      "Epoch 145/150\n",
      "34/34 [==============================] - 0s 341us/step - loss: 6.8868e-04\n",
      "Epoch 146/150\n",
      "34/34 [==============================] - 0s 392us/step - loss: 6.8794e-04\n",
      "Epoch 147/150\n",
      "34/34 [==============================] - 0s 366us/step - loss: 6.8718e-04\n",
      "Epoch 148/150\n",
      "34/34 [==============================] - 0s 339us/step - loss: 6.8644e-04\n",
      "Epoch 149/150\n",
      "34/34 [==============================] - 0s 330us/step - loss: 6.8569e-04\n",
      "Epoch 150/150\n",
      "34/34 [==============================] - 0s 383us/step - loss: 6.8494e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a421316d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def deep_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=17, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    model.layers[0].set_weights([np.zeros([17,8])+0.01, np.zeros([8,])])\n",
    "    model.layers[1].set_weights([np.zeros([8,8])+0.01, np.zeros([8,])])\n",
    "    model.layers[2].set_weights([np.zeros([8,1])+0.01, np.array([0.00])])\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "deepNetwork = deep_model()\n",
    "\n",
    "deepNetwork.fit(X_train, y_train, epochs=150, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on the testing set: 0.001012160792015493\n"
     ]
    }
   ],
   "source": [
    "test_loss = deepNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "print('The loss on the testing set:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the RELU loss function seems to improve the loss ever so slightly more... Perhaps this is due to the fact that the first layer can pick out the significant combinations (normalized) and the second can evaluate the relative significance between them. Regardless, the improvement is not that significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of performance\n",
    "\n",
    "Here we compare the performance of the four models.\n",
    "\n",
    "IMPORTANT: The following cells are dependent on the networks being trained on the test split defined in the earlier cells by\n",
    "\n",
    "````\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=445)\n",
    "````\n",
    "\n",
    "The networks are assumed to be trained for the next section to be accurate.\n",
    "\n",
    "\n",
    "### Comparison of a single test-training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>base</th>\n",
       "      <th>thin</th>\n",
       "      <th>deep</th>\n",
       "      <th>LinReg</th>\n",
       "      <th>Trivial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.196563</td>\n",
       "      <td>0.216993</td>\n",
       "      <td>0.238485</td>\n",
       "      <td>0.238196</td>\n",
       "      <td>0.284242</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.203043</td>\n",
       "      <td>0.229985</td>\n",
       "      <td>0.212877</td>\n",
       "      <td>0.209964</td>\n",
       "      <td>0.224638</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.219613</td>\n",
       "      <td>0.236922</td>\n",
       "      <td>0.215244</td>\n",
       "      <td>0.214599</td>\n",
       "      <td>0.222883</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.269133</td>\n",
       "      <td>0.220646</td>\n",
       "      <td>0.244147</td>\n",
       "      <td>0.243613</td>\n",
       "      <td>0.336534</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.177154</td>\n",
       "      <td>0.268167</td>\n",
       "      <td>0.218040</td>\n",
       "      <td>0.216646</td>\n",
       "      <td>0.208369</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.215174</td>\n",
       "      <td>0.190213</td>\n",
       "      <td>0.207007</td>\n",
       "      <td>0.207519</td>\n",
       "      <td>0.213413</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.256654</td>\n",
       "      <td>0.211731</td>\n",
       "      <td>0.235224</td>\n",
       "      <td>0.235248</td>\n",
       "      <td>0.249962</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.134223</td>\n",
       "      <td>-0.024845</td>\n",
       "      <td>0.017704</td>\n",
       "      <td>0.056105</td>\n",
       "      <td>-1.010888</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.201903</td>\n",
       "      <td>0.229205</td>\n",
       "      <td>0.195365</td>\n",
       "      <td>0.194550</td>\n",
       "      <td>0.165244</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.285952</td>\n",
       "      <td>0.238172</td>\n",
       "      <td>0.228763</td>\n",
       "      <td>0.227901</td>\n",
       "      <td>0.261675</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.171053</td>\n",
       "      <td>0.245538</td>\n",
       "      <td>0.207598</td>\n",
       "      <td>0.207071</td>\n",
       "      <td>0.194868</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.232762</td>\n",
       "      <td>0.232879</td>\n",
       "      <td>0.243578</td>\n",
       "      <td>0.243248</td>\n",
       "      <td>0.219523</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.221781</td>\n",
       "      <td>0.185481</td>\n",
       "      <td>0.213136</td>\n",
       "      <td>0.212164</td>\n",
       "      <td>0.249120</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.236921</td>\n",
       "      <td>0.231878</td>\n",
       "      <td>0.208358</td>\n",
       "      <td>0.208515</td>\n",
       "      <td>0.174583</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.226371</td>\n",
       "      <td>0.206084</td>\n",
       "      <td>0.214252</td>\n",
       "      <td>0.215913</td>\n",
       "      <td>0.249952</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.222080</td>\n",
       "      <td>0.192309</td>\n",
       "      <td>0.202878</td>\n",
       "      <td>0.203804</td>\n",
       "      <td>0.243984</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.248240</td>\n",
       "      <td>0.192699</td>\n",
       "      <td>0.227918</td>\n",
       "      <td>0.228332</td>\n",
       "      <td>0.222958</td>\n",
       "      <td>0.215838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual      base      thin      deep    LinReg   Trivial\n",
       "0   0.196563  0.216993  0.238485  0.238196  0.284242  0.215838\n",
       "1   0.203043  0.229985  0.212877  0.209964  0.224638  0.215838\n",
       "2   0.219613  0.236922  0.215244  0.214599  0.222883  0.215838\n",
       "3   0.269133  0.220646  0.244147  0.243613  0.336534  0.215838\n",
       "4   0.177154  0.268167  0.218040  0.216646  0.208369  0.215838\n",
       "5   0.215174  0.190213  0.207007  0.207519  0.213413  0.215838\n",
       "6   0.256654  0.211731  0.235224  0.235248  0.249962  0.215838\n",
       "7   0.134223 -0.024845  0.017704  0.056105 -1.010888  0.215838\n",
       "8   0.201903  0.229205  0.195365  0.194550  0.165244  0.215838\n",
       "9   0.285952  0.238172  0.228763  0.227901  0.261675  0.215838\n",
       "10  0.171053  0.245538  0.207598  0.207071  0.194868  0.215838\n",
       "11  0.232762  0.232879  0.243578  0.243248  0.219523  0.215838\n",
       "12  0.221781  0.185481  0.213136  0.212164  0.249120  0.215838\n",
       "13  0.236921  0.231878  0.208358  0.208515  0.174583  0.215838\n",
       "14  0.226371  0.206084  0.214252  0.215913  0.249952  0.215838\n",
       "15  0.222080  0.192309  0.202878  0.203804  0.243984  0.215838\n",
       "16  0.248240  0.192699  0.227918  0.228332  0.222958  0.215838"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate test and training data splits\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=445)\n",
    "\n",
    "# Get predicted values for networks\n",
    "base_pred = baseNetwork.predict(X_test)\n",
    "thin_pred = thinNetwork.predict(X_test)\n",
    "deep_pred = deepNetwork.predict(X_test)\n",
    "\n",
    "# Predicted values for linear regression\n",
    "reg = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n",
    "lnrg_pred = reg.predict(X_test)\n",
    "\n",
    "# Trivial predictor as mean of training data\n",
    "triv_pred = np.ones([len(y_test), 1]) * np.mean(y_train)\n",
    "\n",
    "# Showing predictions\n",
    "predictions = np.stack([y_test.reshape(-1), base_pred.reshape(-1), thin_pred.reshape(-1), \n",
    "                        deep_pred.reshape(-1), lnrg_pred.reshape(-1), triv_pred.reshape(-1)])\n",
    "\n",
    "pred = pd.DataFrame(predictions.T, columns=['Actual', 'base', 'thin', 'deep', 'LinReg', 'Trivial'])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 17)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference = predictions - predictions[0]\n",
    "\n",
    "labels = np.zeros(len(y_test))\n",
    "labels = np.stack([labels, labels+1, labels+2, labels+3, labels+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.scatter(difference[1], labels[4], label='Base')\n",
    "ax1.scatter(difference[2], labels[3], label='Thin')\n",
    "ax1.scatter(difference[3], labels[2], label='Deep')\n",
    "ax1.scatter(difference[4], labels[1], label='LinReg')\n",
    "ax1.scatter(difference[5], labels[0], label='Trivial')\n",
    "plt.legend()\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.scatter(difference[1], labels[4], label='Base')\n",
    "ax2.scatter(difference[2], labels[3], label='Thin')\n",
    "ax2.scatter(difference[3], labels[2], label='Deep')\n",
    "ax2.scatter(difference[4], labels[1], label='LinReg')\n",
    "ax2.scatter(difference[5], labels[0], label='Trivial')\n",
    "ax2.set_xlim([-0.1, 0.1])\n",
    "\n",
    "plt.suptitle('Difference on each test point (right zoomed)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[base_mse, thin_mse, deep_mse, lnrg_mse, triv_mse] = [0.05652857 0.03823828 0.03181447 0.28012531 0.03621467]\n"
     ]
    }
   ],
   "source": [
    "# Calculate (root) mean squared errors\n",
    "base_mse = np.sqrt(np.mean((base_pred - y_test) ** 2))\n",
    "thin_mse = np.sqrt(np.mean((thin_pred - y_test) ** 2))\n",
    "deep_mse = np.sqrt(np.mean((deep_pred - y_test) ** 2))\n",
    "lnrg_mse = np.sqrt(np.mean((lnrg_pred - y_test) ** 2))\n",
    "triv_mse = np.sqrt(np.mean((triv_pred - y_test) ** 2))\n",
    "\n",
    "RMSE = np.array([base_mse, thin_mse, deep_mse, lnrg_mse, triv_mse])\n",
    "\n",
    "print('[base_mse, thin_mse, deep_mse, lnrg_mse, triv_mse] =', RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the trivial predictor remains relatively accurate (more so than most other approaches). This is again because the growth rates tended to be very similar although the parameters varied widely.\n",
    "\n",
    "Interestingly, the performance as characterized by the RMSE as given above is quite different from the losses determined by the Model.evaluate() method. I'm not exactly sure why this difference occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Calculated</th>\n",
       "      <th>Evaluate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>base</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.056529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>thin</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.038238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>deep</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.031814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Calculated  Evaluate\n",
       "base    0.003195  0.056529\n",
       "thin    0.001462  0.038238\n",
       "deep    0.001012  0.031814"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_eval = baseNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "thin_eval = thinNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "deep_eval = deepNetwork.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "compare = np.array([[base_eval, thin_eval, deep_eval], [base_mse, thin_mse, deep_mse]])\n",
    "compdf = pd.DataFrame(compare.T, index=['base', 'thin', 'deep'], columns=['Calculated', 'Evaluate'])\n",
    "compdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing performance on an ensemble of test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test set number 0\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 0 complete.\n",
      "\n",
      "Starting test set number 1\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 1 complete.\n",
      "\n",
      "Starting test set number 2\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 2 complete.\n",
      "\n",
      "Starting test set number 3\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 3 complete.\n",
      "\n",
      "Starting test set number 4\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 4 complete.\n",
      "\n",
      "Starting test set number 5\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 5 complete.\n",
      "\n",
      "Starting test set number 6\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 6 complete.\n",
      "\n",
      "Starting test set number 7\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 7 complete.\n",
      "\n",
      "Starting test set number 8\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 8 complete.\n",
      "\n",
      "Starting test set number 9\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 9 complete.\n",
      "\n",
      "Starting test set number 10\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 10 complete.\n",
      "\n",
      "Starting test set number 11\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 11 complete.\n",
      "\n",
      "Starting test set number 12\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 12 complete.\n",
      "\n",
      "Starting test set number 13\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 13 complete.\n",
      "\n",
      "Starting test set number 14\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 14 complete.\n",
      "\n",
      "Starting test set number 15\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 15 complete.\n",
      "\n",
      "Starting test set number 16\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 16 complete.\n",
      "\n",
      "Starting test set number 17\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 17 complete.\n",
      "\n",
      "Starting test set number 18\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 18 complete.\n",
      "\n",
      "Starting test set number 19\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 19 complete.\n",
      "\n",
      "Starting test set number 20\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 20 complete.\n",
      "\n",
      "Starting test set number 21\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 21 complete.\n",
      "\n",
      "Starting test set number 22\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 22 complete.\n",
      "\n",
      "Starting test set number 23\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 23 complete.\n",
      "\n",
      "Starting test set number 24\n",
      "Training base network...\n",
      "Training thin network...\n",
      "Training deep network...\n",
      "Calcuating errors...\n",
      "Test set 24 complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will take a long time to run\n",
    "lnrg = []\n",
    "base = []\n",
    "thin = []\n",
    "deep = []\n",
    "triv = []\n",
    "\n",
    "for i in range(25):\n",
    "    print('Starting test set number', i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33)\n",
    "    reg = LinearRegression(fit_intercept=True).fit(X_train, y_train)\n",
    "    \n",
    "    print('Training base network...')\n",
    "    baseNetwork = base_model()\n",
    "    baseNetwork.fit(X_train, y_train, epochs=100, batch_size=5, shuffle=False, verbose=0)\n",
    "\n",
    "    print('Training thin network...')\n",
    "    thinNetwork = thin_model()\n",
    "    thinNetwork.fit(X_train, y_train, epochs=100, batch_size=5, shuffle=False, verbose=0)\n",
    "    \n",
    "    print('Training deep network...')\n",
    "    deepNetwork = deep_model()\n",
    "    deepNetwork.fit(X_train, y_train, epochs=100, batch_size=5, shuffle=False, verbose=0)\n",
    "    \n",
    "    print('Calcuating errors...')\n",
    "    base_pred = baseNetwork.predict(X_test)\n",
    "    thin_pred = thinNetwork.predict(X_test)\n",
    "    deep_pred = deepNetwork.predict(X_test)\n",
    "    lnrg_pred = reg.predict(X_test)\n",
    "    \n",
    "    base_mse = np.sqrt(np.mean((base_pred - y_test) ** 2))\n",
    "    thin_mse = np.sqrt(np.mean((thin_pred - y_test) ** 2))\n",
    "    deep_mse = np.sqrt(np.mean((deep_pred - y_test) ** 2))\n",
    "    lnrg_mse = np.sqrt(np.mean((lnrg_pred - y_test) ** 2))\n",
    "    triv_mse = np.sqrt(np.mean((np.mean(y_train) - y_test) ** 2))\n",
    "    \n",
    "    base = np.append(base, base_mse)\n",
    "    thin = np.append(thin, thin_mse)\n",
    "    deep = np.append(deep, deep_mse)\n",
    "    lnrg = np.append(lnrg, lnrg_mse)\n",
    "    triv = np.append(triv, triv_mse)\n",
    "    print('Test set', i, 'complete.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinReg</th>\n",
       "      <th>Trivial</th>\n",
       "      <th>Base</th>\n",
       "      <th>Thin</th>\n",
       "      <th>Deep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.081229</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>0.095360</td>\n",
       "      <td>0.033980</td>\n",
       "      <td>0.033483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.073918</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.083533</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.004637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.026578</td>\n",
       "      <td>0.029751</td>\n",
       "      <td>0.032261</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.022426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.033802</td>\n",
       "      <td>0.032917</td>\n",
       "      <td>0.046692</td>\n",
       "      <td>0.030951</td>\n",
       "      <td>0.031320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.041252</td>\n",
       "      <td>0.035791</td>\n",
       "      <td>0.060578</td>\n",
       "      <td>0.034204</td>\n",
       "      <td>0.033362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.107965</td>\n",
       "      <td>0.037198</td>\n",
       "      <td>0.089558</td>\n",
       "      <td>0.038597</td>\n",
       "      <td>0.036286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.289109</td>\n",
       "      <td>0.045188</td>\n",
       "      <td>0.390824</td>\n",
       "      <td>0.042023</td>\n",
       "      <td>0.042040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LinReg    Trivial       Base       Thin       Deep\n",
       "count  25.000000  25.000000  25.000000  25.000000  25.000000\n",
       "mean    0.081229   0.035533   0.095360   0.033980   0.033483\n",
       "std     0.073918   0.003601   0.083533   0.005264   0.004637\n",
       "min     0.026578   0.029751   0.032261   0.021918   0.022426\n",
       "25%     0.033802   0.032917   0.046692   0.030951   0.031320\n",
       "50%     0.041252   0.035791   0.060578   0.034204   0.033362\n",
       "75%     0.107965   0.037198   0.089558   0.038597   0.036286\n",
       "max     0.289109   0.045188   0.390824   0.042023   0.042040"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = np.stack([lnrg, triv, base, thin, deep])\n",
    "\n",
    "errdf = pd.DataFrame(error.T, columns=['LinReg', 'Trivial', 'Base', 'Thin', 'Deep'])\n",
    "errdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = errdf.plot.hist(bins=100, alpha=0.5, title='Distribution of MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = errdf.plot.hist(bins=200, alpha=0.5, title='Distribution of MSE', xlim=(0,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = np.zeros(len(lnrg))\n",
    "labels = np.stack([labels, labels+1, labels+2, labels+3, labels+4])\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.scatter(lnrg, labels[4], label='LinReg')\n",
    "ax1.scatter(triv, labels[3], label='Trivial')\n",
    "ax1.scatter(base, labels[2], label='Base')\n",
    "ax1.scatter(thin, labels[1], label='Thin')\n",
    "ax1.scatter(deep, labels[0], label='Deep')\n",
    "plt.legend()\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.scatter(lnrg, labels[4], label='LinReg')\n",
    "ax2.scatter(triv, labels[3], label='Trivial')\n",
    "ax2.scatter(base, labels[2], label='Base')\n",
    "ax2.scatter(thin, labels[1], label='Thin')\n",
    "ax2.scatter(deep, labels[0], label='Deep')\n",
    "ax2.set_xlim([0.01, 0.05])\n",
    "\n",
    "plt.suptitle('Distribution of RMSE loss (right zoomed)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of performance\n",
    "\n",
    "This analysis gives that percent error of the prediction for the network solution tended to be in about the 10% error range. The networks provided a much larger increase in performance accuracy compared to a linear regression, and the introduction, and was seemingly able to capture the information of the data set better since it outperformaed the trivial predictor roughly half of the time.\n",
    "\n",
    "The error margin is still rather large, but this is perhaps due to not only an error in the computational power of the networks but also a general failiure of the model to capture all data. For instance, the linear regression used to determine the \"infection rate\" $r$ within each state was not perfect. Each computation of the rate had been subject to a varied number of data points (since the infection started at different times).\n",
    "\n",
    "Furthermore, the varying fidelity of the underlying model as an exponential growth (while not too incorrect within the first half of the data set) is not always correct, especially with the generally small amount of data to fit to (especially in states where infection rates are small and stochastic noise can be significant on the fit), or states wehre the infection started very early on (and quarantine measures had already started taking effect before April 12). Beyond the model likely being too simplistic, the data used to train was not necessarily representative of the whole picture. For isntance, the attitude of residents might be more significant, regardless of the state-level action, and yet could not be caputred as data for the model. \n",
    "\n",
    "In general, the performance of the network shows that it had the capability to capture the information associated with the outlier data points better than a trivial prediciton (where the outliers are much more significant) or linear regression. I think the large margin of error that the network produces boils down to several sources:\n",
    "1. The model was overly simplistic: exponential growth does not necessarily model the system exactly.\n",
    "2. The data was not complete: I only captured a small subset of actions that the state governments could hve taken. It is liekly that there are many other factors that could not be caputred directly and were not taken into account.\n",
    "3. The data was limited to a small size: there may not have been enough data to properly fit to the model. Truly outlier data (due to the stochastic nature of such systems) appears much more significantly in such a small data set of only 51 points.\n",
    "\n",
    "To that end, it is likely that the network was undertrained, primarily in the sense that it had not been given enough information to begin with to properly \"learn\" all features of the dataset. Given the dataset provided, it is clear that the network was more than capable of reaching a global minimum in the loss function landscape, but the issue with the model is that the loss function profile (on all the data) was just not a completely accurate representation of what the data really is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
